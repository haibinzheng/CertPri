{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90b61167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from progressbar import ProgressBar\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import backend as BE\n",
    "from tensorflow.keras.models import Model\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from Integrated_Gradients_algorithm import *\n",
    "\n",
    "# tf.enable_eager_execution(\n",
    "#     config=None,\n",
    "#     device_policy=None,\n",
    "#     execution_mode=None\n",
    "# )\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "# tf.config.run_functions_eagerly(True)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "config=tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True   #不全部占满显存, 按需分配\n",
    "sess=tf.compat.v1.Session(config=config)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "686692ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras\n",
    "def get_loss_gradients(img_input, model, target_one_hot, from_logits=False):\n",
    "    images = tf.cast(img_input, tf.float32)\n",
    "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=from_logits)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(images)\n",
    "        preds = model(images)\n",
    "        loss = cce(target_one_hot, preds)\n",
    "#         top_class = preds[:, top_pred_idx]\n",
    "\n",
    "    grads = tape.gradient(loss, images)\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66ca4a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(predict_label, ground_truth=None):\n",
    "    for i in predict_label[0]:\n",
    "        if i[0] == ground_truth:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def softmax( f ):\n",
    "    # instead: first shift the values of f so that the highest number is 0:\n",
    "    s = f-np.max(f) # f becomes [-666, -333, 0]\n",
    "    return np.exp(s) / np.sum(np.exp(s))  # safe to do, gives the correct answer\n",
    "\n",
    "def get_APFD(Gini_indexs, ground_truth_label, predicted_confidence, top_set=None):\n",
    "    o_i = 0\n",
    "    pbar = ProgressBar()\n",
    "    wrong_num = 0\n",
    "    wrong_index = []\n",
    "    for i in pbar(range(0, len(Gini_indexs))):\n",
    "        if top_set is not None:\n",
    "            if not get_acc(predict_label=decode_predictions(predicted_confidence[Gini_indexs[i]], top=top_set), \n",
    "                           ground_truth=ground_truth_label[Gini_indexs[i]]):\n",
    "                o_i = o_i+i\n",
    "#                 print(i, o_i)\n",
    "                wrong_num = wrong_num+1\n",
    "                wrong_index.append(i)\n",
    "        else:\n",
    "            if np.argmax(ground_truth_label[Gini_indexs[i]]) != np.argmax(predicted_confidence[Gini_indexs[i]]):\n",
    "                o_i = o_i+i\n",
    "                wrong_num = wrong_num+1\n",
    "                wrong_index.append(i)\n",
    "    APFD = 1 - o_i/(len(Gini_indexs)*wrong_num) + 1/(2*len(Gini_indexs))\n",
    "    return APFD, wrong_num, wrong_index\n",
    "\n",
    "def get_RAUC(Gini_indexs, ground_truth_label, predicted_confidence, top_set=None):\n",
    "    pre_y_axis = []\n",
    "    o_i = 0\n",
    "    wrong_num = 0\n",
    "    pbar = ProgressBar()\n",
    "    for i in pbar(range(0, len(Gini_indexs))):\n",
    "        if top_set is not None:\n",
    "            if not get_acc(predict_label=decode_predictions(predicted_confidence[Gini_indexs[i]], top=top_set), \n",
    "                           ground_truth=ground_truth_label[Gini_indexs[i]]):  \n",
    "                o_i = o_i+1\n",
    "                wrong_num = wrong_num+1\n",
    "                pre_y_axis.append(o_i)\n",
    "            else:\n",
    "                pre_y_axis.append(o_i)\n",
    "        else:\n",
    "            if np.argmax(ground_truth_label[Gini_indexs[i]]) != np.argmax(predicted_confidence[Gini_indexs[i]]):\n",
    "                o_i = o_i+1\n",
    "                wrong_num = wrong_num+1\n",
    "                pre_y_axis.append(o_i)\n",
    "            else:\n",
    "                pre_y_axis.append(o_i)\n",
    "    true_y_axis = wrong_num*(len(Gini_indexs)-wrong_num) + (wrong_num+1)*wrong_num/2\n",
    "    RAUC = np.sum(pre_y_axis)/true_y_axis\n",
    "#     print(\"RAUC: \", RAUC)\n",
    "    return RAUC, len(Gini_indexs), wrong_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cc8fa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "992b6412",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()\n",
    "X_train = np.reshape(X_train,[-1,28*28])\n",
    "X_test = np.reshape(X_test,[-1,28*28])\n",
    "Y_train = to_categorical(Y_train,10)\n",
    "Y_test = to_categorical(Y_test,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc88dd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_shape = 28*28\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Dense(input_shape=(input_shape,), units=input_shape, activation='relu'))\n",
    "# model.add(Dense(units=128, activation='relu'))\n",
    "# model.add(Dense(units=32, activation='relu'))\n",
    "# model.add(Dense(units=10, activation='softmax'))\n",
    "# # model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # # model training\n",
    "# # model.fit(X_train, Y_train, epochs=1, batch_size=512)\n",
    "\n",
    "# # MODEL_PATH = \"/public/liujiawei/huawei/ZHB/ADF-master/models/\"\n",
    "# # model.save(MODEL_PATH+\"fmnist_FC4_bad.h5\")\n",
    "\n",
    "# # del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85833ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"/public/liujiawei/huawei/ZHB/ADF-master/models/\"\n",
    "model = load_model(MODEL_PATH+\"fmnist_FC4_bad.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c65d88ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/public/liujiawei/anaconda3/envs/ZHB_env/lib/python3.6/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7201\n"
     ]
    }
   ],
   "source": [
    "loss_acc = model.evaluate(X_test, Y_test, batch_size=128)\n",
    "print('accuracy', loss_acc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9252c53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n",
      "100% |########################################################################|\n",
      "100% |########################################################################|\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APFD: 0.7416754385964912\n",
      "RAUC: 0.863840837525048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "img_num = 1000\n",
    "X_test = X_test[0:img_num]\n",
    "Y_test = Y_test[0:img_num]\n",
    "Gini = []\n",
    "predicted_confidence = []\n",
    "index1 = 0\n",
    "index2 = 0\n",
    "pbar = ProgressBar()\n",
    "for x_tmp in pbar(X_test):\n",
    "    per_tmp = model.predict(x_tmp.reshape([-1,28*28]))\n",
    "    label_tmp = np.argmax(per_tmp)\n",
    "    if label_tmp == np.argmax(Y_test[index1]):\n",
    "        index2 = index2 +1\n",
    "    index1 = index1 + 1\n",
    "    Gini_tmp = 1-np.sum(per_tmp*per_tmp)\n",
    "#     Gini_tmp = -np.sum(per_tmp*np.log2(per_tmp))\n",
    "    Gini.append(Gini_tmp)\n",
    "    predicted_confidence.append(per_tmp)\n",
    "    \n",
    "indexs = np.argsort(Gini)\n",
    "indexs = indexs[::-1]\n",
    "\n",
    "APFD,_,_ = get_APFD(Gini_indexs=indexs, ground_truth_label=Y_test, \n",
    "                predicted_confidence=np.array(predicted_confidence))\n",
    "print('APFD:', APFD)\n",
    "RAUC,_,_ = get_RAUC(Gini_indexs=indexs, ground_truth_label=Y_test, \n",
    "                predicted_confidence=np.array(predicted_confidence))\n",
    "print('RAUC:', RAUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "029ebacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test number: 1000 \t misclassified number: 285\n"
     ]
    }
   ],
   "source": [
    "print('test number:', index1, '\\t misclassified number:', index1-index2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7497d0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# o_i = 0\n",
    "# for i in range(0, (index1)):\n",
    "#     x_tmp = X_test[indexs[i]].reshape([-1,28*28])\n",
    "#     y_tmp = Y_test[indexs[i]]\n",
    "#     per_tmp = model.predict(x_tmp)\n",
    "#     if np.argmax(y_tmp)!=np.argmax(per_tmp):\n",
    "#         o_i = o_i+i\n",
    "# APFD = 1-o_i/(index1*(index1-index2))+1/(2*index1)\n",
    "# print(APFD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a5b890e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 784)               615440    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 720,378\n",
      "Trainable params: 720,378\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b391129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use one layer's activation\n",
    "# get_activations = BE.function(inputs=model.inputs[0], outputs=model.layers[-3].output[:,:])\n",
    "# Gini_act = []\n",
    "# pbar = ProgressBar()\n",
    "# for x_tmp in pbar(X_test):    \n",
    "# #     # 使用attention机制\n",
    "# #     x_act = softmax(get_activations(x_tmp.reshape([-1,28*28])))\n",
    "# #     att1 = np.dot(np.transpose(x_act), x_act)\n",
    "# #     att_weight = np.sum(att1, axis = 1).reshape(np.shape(x_act))\n",
    "# #     x_act_ = softmax(x_act*att_weight)\n",
    "# #     Gini_tmp = 1-np.sum(x_act_*x_act_)\n",
    "# #     Gini_tmp = -np.sum(x_act_*np.log2(x_act_))\n",
    "# #     Gini_act.append(Gini_tmp)\n",
    "    \n",
    "#     # 未使用attention机制\n",
    "#     x_act = softmax(get_activations(x_tmp.reshape([-1,28*28])))\n",
    "# #     Gini_tmp = 1-np.sum(x_act*x_act)\n",
    "#     Gini_tmp = -np.sum(x_act*np.log2(x_act))\n",
    "#     Gini_act.append(Gini_tmp)\n",
    "\n",
    "# # # use all layers' activation\n",
    "# # pbar = ProgressBar()\n",
    "# # get_activations1 = BE.function(inputs=model.inputs[0], outputs=model.layers[-1].output[:,:])\n",
    "# # get_activations2 = BE.function(inputs=model.inputs[0], outputs=model.layers[-2].output[:,:])\n",
    "# # get_activations3 = BE.function(inputs=model.inputs[0], outputs=model.layers[-3].output[:,:])\n",
    "# # Gini_act = []\n",
    "# # for x_tmp in pbar(X_test):\n",
    "# #     x_act1 = softmax(get_activations1(x_tmp.reshape([-1,28*28])))\n",
    "# #     x_act2 = softmax(get_activations2(x_tmp.reshape([-1,28*28])))\n",
    "# #     x_act3 = softmax(get_activations3(x_tmp.reshape([-1,28*28])))\n",
    "# # #     Gini_tmp = 1-np.sum(x_act3*x_act3)\n",
    "# # #     Gini_tmp = 1-(np.sum(x_act1*x_act1)+np.sum(x_act2*x_act2)+np.sum(x_act3*x_act3))/3\n",
    "# #     Gini_tmp = -np.sum(x_act3*np.log2(x_act3))\n",
    "# # #     Gini_tmp = (-np.sum(x_act1*np.log2(x_act1))-np.sum(x_act2*np.log2(x_act2))-np.sum(x_act3*np.log2(x_act3)))/3\n",
    "# #     Gini_act.append(Gini_tmp)\n",
    "    \n",
    "# indexs = np.argsort(Gini_act)\n",
    "# indexs = indexs[::-1]\n",
    "# APFD = get_APFD(Gini_indexs=indexs, ground_truth_label=Y_test, \n",
    "#                 predicted_confidence=np.array(predicted_confidence))\n",
    "# print('APFD:', APFD)\n",
    "# RAUC,_,_ = get_RAUC(Gini_indexs=indexs, ground_truth_label=Y_test, \n",
    "#                 predicted_confidence=np.array(predicted_confidence))\n",
    "# print('RAUC:', RAUC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cca10216",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     输出层对模型的某一层求导\n",
    "def get_hidden_layer_gradient(x_input, model, pre_conf, layers_names):\n",
    "    top_pred_idx = np.argmax(pre_conf)\n",
    "    hidden_layer = model.get_layer(layers_names).output\n",
    "    grads = BE.gradients(loss = model.layers[-1].output[:, top_pred_idx], variables = hidden_layer)\n",
    "    get_gradients = BE.function(inputs=model.inputs[0], outputs=grads)\n",
    "    layer_grad = get_gradients(x_input)[0]\n",
    "    return layer_grad\n",
    "    \n",
    "#     损失函数对模型的某一层求导\n",
    "def get_hidden_layer_loss_gradient(x_input, model, pre_conf, layers_names):\n",
    "    top_pred_idx = np.argmax(pre_conf)\n",
    "    num_class = np.shape(pre_conf)[-1]\n",
    "    hidden_layer = model.get_layer(layers_names).output\n",
    "    loss = BE.categorical_crossentropy(to_categorical(top_pred_idx, num_class), model.layers[-1].output[:,:][0])\n",
    "#     loss = BE.categorical_crossentropy(to_categorical(label_tmp, num_class), pre_conf[0]) # 用 preds[0]会报NoneType错\n",
    "    grads = BE.gradients(loss = loss, variables = hidden_layer)\n",
    "    get_gradients = BE.function(inputs=model.inputs[0], outputs=grads)\n",
    "    layer_grad = get_gradients(x_input)[0]\n",
    "    return layer_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa532ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7415bc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14% |##########                                                              |\r"
     ]
    }
   ],
   "source": [
    "# use one layer's activation\n",
    "pbar = ProgressBar()\n",
    "# layer_pos = -2\n",
    "# get_activations = BE.function(inputs=model.inputs[0], outputs=model.layers[layer_pos].output[:,:])\n",
    "grads_pre1 = []\n",
    "grads_pre2 = []\n",
    "grads_CE = []\n",
    "grads_CE2 = []\n",
    "\n",
    "grads_Out_HL = []\n",
    "grads_Loss_HL = []\n",
    "feature_HL = []\n",
    "\n",
    "num_class = 10\n",
    "# predicted_confidence = []\n",
    "# ground_truth_label = []\n",
    "for x_tmp in pbar(X_test):\n",
    "    x_tmp = x_tmp.reshape(1, -1)\n",
    "    preds = model.predict(x_tmp)\n",
    "    label_tmp = np.argmax(preds)\n",
    "    layers_names = 'dense_2'\n",
    "    \n",
    "# #     损失函数对模型的某一层求导\n",
    "#     layer_grads = get_hidden_layer_loss_gradient(x_input=x_tmp, model=model, pre_conf=preds, layers_names=layers_names)\n",
    "#     grads_CE.append(layer_grads)\n",
    "        \n",
    "# #     输出层对模型的某一层求导\n",
    "#     layer_grads = get_hidden_layer_gradient(x_input, model, pre_conf, layers_names)\n",
    "        \n",
    "        \n",
    "#     hidden_layer = model.get_layer(layers_names).output\n",
    "# #     cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "# #     with tf.GradientTape() as tape:\n",
    "# #         tape.watch(hidden_layer)\n",
    "# #         preds = model(x_tmp)\n",
    "# #         loss = cce(to_categorical(top_pred_idx, num_class), preds)\n",
    "# # #         top_class = preds[:, top_pred_idx]\n",
    "# #     grads = tape.gradient(top_class, tf.cast(x_tmp, tf.float32))\n",
    "# #     grads = BE.gradients(loss = model.layers[-1].output[:,label_tmp], variables = hidden_layer)\n",
    "# #     grads = BE.gradients(loss = cce(to_categorical(label_tmp, num_class), preds[0]), variables = hidden_layer)\n",
    "\n",
    "#     损失函数对模型的某一层求导\n",
    "    model_hidden_layer = Model(inputs=model.input, outputs=model.get_layer(layers_names).output)\n",
    "    hidden_layer = model.get_layer(layers_names).output\n",
    "    loss = BE.categorical_crossentropy( to_categorical(label_tmp, num_class), model.layers[-1].output[:,:][0] )\n",
    "    grads = BE.gradients(loss = loss, variables = hidden_layer)\n",
    "    get_gradients_loss = BE.function(inputs=model.inputs[0], outputs=grads)\n",
    "    x_grad = get_gradients_loss(x_tmp)[0]\n",
    "    features = model_hidden_layer.predict(x_tmp)\n",
    "    grads_Loss_HL.append(x_grad)\n",
    "    feature_HL.append(features)\n",
    "    \n",
    "    \n",
    "\n",
    "#     输出层对模型的某一层求导\n",
    "    hidden_layer = model.get_layer(layers_names).output\n",
    "    grads = BE.gradients(loss = model.layers[-1].output[:,label_tmp], variables = hidden_layer)\n",
    "    get_gradients_Out = BE.function(inputs=model.inputs[0], outputs=grads)\n",
    "    x_grad = get_gradients_Out(x_tmp)[0]\n",
    "    grads_Out_HL.append(x_grad)\n",
    "    \n",
    "#     target_one_hot = np.ones(num_class)*(1.0/num_class)\n",
    "#     grads = get_loss_gradients(img_input=x_tmp, model=model, target_one_hot=target_one_hot)\n",
    "#     grads = grads.numpy()\n",
    "#     grads = np.sum(np.abs(grads))\n",
    "#     grads_CE.append(grads)\n",
    "    \n",
    "#     # 计算每一类的梯度\n",
    "#     grads_sum = np.zeros(num_class)\n",
    "#     for label_i in range(num_class):\n",
    "# #         target_one_hot = np.reshape(to_categorical(label_i, num_class), (-1, num_class))\n",
    "# #         grads = get_loss_gradients(img_input=x_tmp, model=model, target_one_hot=target_one_hot)\n",
    "\n",
    "#         grads = get_gradients(img_input=x_tmp, model=model, top_pred_idx=label_i)\n",
    "# #         grads = grads.numpy()\n",
    "#         grads = np.sum(np.abs(grads))\n",
    "# #         grads = np.sum(grads)\n",
    "#         grads_sum[label_i] = grads*grads\n",
    "#     grads_CE.append(grads_sum)\n",
    "    \n",
    "    #     使用预测标签当做真实标签验证\n",
    "    label1 = np.argmax(preds[0])\n",
    "    grads = get_gradients(img_input=x_tmp, model=model, top_pred_idx=label1)\n",
    "#     grads = grads.numpy()\n",
    "#     grads = BE.sum(BE.abs(grads))\n",
    "    grads_pre1.append(grads)\n",
    "    \n",
    "    target_one_hot = np.reshape(to_categorical(label1, num_class), (-1, num_class))\n",
    "    grads = get_loss_gradients(img_input=x_tmp, model=model, target_one_hot=target_one_hot)\n",
    "#     grads = grads.numpy()\n",
    "#     grads = BE.sum(BE.abs(grads))\n",
    "    grads_CE.append(grads)\n",
    "    \n",
    "    preds[0][label1] = 0\n",
    "    label2 = np.argmax(preds[0])\n",
    "    grads = get_gradients(img_input=x_tmp, model=model, top_pred_idx=label2)\n",
    "#     grads = grads.numpy()\n",
    "#     grads = BE.sum(BE.abs(grads))\n",
    "    grads_pre2.append(grads)\n",
    "    \n",
    "    target_one_hot = np.reshape(to_categorical(label2, num_class), (-1, num_class))\n",
    "    grads = get_loss_gradients(img_input=x_tmp, model=model, target_one_hot=target_one_hot)\n",
    "#     grads = grads.numpy()\n",
    "#     grads = BE.sum(BE.abs(grads))\n",
    "    grads_CE2.append(grads)\n",
    "\n",
    "#     # 使用attention机制\n",
    "#     x_act = softmax(get_activations(x_tmp.reshape([-1,28*28])))\n",
    "#     att1 = np.dot(np.transpose(x_act), x_act)\n",
    "#     att_weight = np.sum(att1, axis = 1).reshape(np.shape(x_act))\n",
    "#     x_act_ = softmax(x_act*att_weight)\n",
    "#     Gini_tmp = 1-np.sum(x_act_*x_act_)\n",
    "#     Gini_act.append(Gini_tmp)\n",
    "    \n",
    "#     predicted_confidence.append(model.predict(x_tmp.reshape([-1,28*28])))\n",
    "#      # 未使用attention机制\n",
    "#     x_act = softmax(get_activations(x_tmp.reshape([-1,28*28])))\n",
    "#     Gini_tmp = 1-np.sum(x_act*x_act)\n",
    "#     Gini_act.append(Gini_tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5941c463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 32])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "88847a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n"
     ]
    }
   ],
   "source": [
    "# Gini_act = []\n",
    "# # grads_CE = np.array(grads_CE)\n",
    "# for i in range(len(grads_CE)):\n",
    "#     Gini_act.append(  1-np.sum(softmax(grads_CE[i]) * softmax(grads_CE[i])) )\n",
    "\n",
    "Gini_act =[]\n",
    "pbar = ProgressBar()\n",
    "for i in pbar(range(len(grads_CE))):\n",
    "#     x_act = np.sum(np.abs(grads_CE[i]))\n",
    "    x_act = np.sum(grads_CE[i]*grads_CE[i])\n",
    "#     print(i,':',x_act)\n",
    "    Gini_act.append(x_act)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d8d0cea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(292,)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gini_act = np.array(Gini_act)\n",
    "# Gini_act.shape\n",
    "# print(np.max(Gini_act), np.max(grads_CE))\n",
    "np.shape(Gini_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "78aeb139",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n",
      "100% |########################################################################|\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APFD: 0.6365729806329712\n",
      "RAUC: 0.7444952222683838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Gini_act = np.array(Gini_act)\n",
    "# indexs = np.argsort(Gini_act[:,3]*Gini_act[:,3])\n",
    "indexs = np.argsort(Gini_act)\n",
    "indexs = indexs[::-1]\n",
    "APFD,_,wrong_index = get_APFD(Gini_indexs=indexs, ground_truth_label=Y_test, \n",
    "                predicted_confidence=np.array(predicted_confidence))\n",
    "print('APFD:', APFD)\n",
    "RAUC,_,_ = get_RAUC(Gini_indexs=indexs, ground_truth_label=Y_test, \n",
    "                predicted_confidence=np.array(predicted_confidence))\n",
    "print('RAUC:', RAUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "aa31f940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ground_truth_cluster = np.zeros([len(result)], dtype=np.int32)\n",
    "ground_truth_cluster[wrong_index]=1\n",
    "# print(ground_truth_cluster)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans,AffinityPropagation, SpectralClustering\n",
    "\n",
    "# 读取数据\n",
    "data = np.squeeze(grads_CE, axis = 1)\n",
    "# 聚类数量\n",
    "k = 2\n",
    "# 训练模型\n",
    "model_kmeans = KMeans(n_clusters=k)\n",
    "model_kmeans.fit(data)\n",
    "# 分类中心点坐标\n",
    "centers = model_kmeans.cluster_centers_\n",
    "# 预测结果\n",
    "result = model_kmeans.predict(data)\n",
    "\n",
    "# 用不同的颜色绘制数据点\n",
    "mark = ['or', 'og']\n",
    "for i, d in enumerate(data):\n",
    "    plt.plot(d[0], d[1], mark[ground_truth_cluster[i]])\n",
    "# 画出各个分类的中心点\n",
    "mark = ['*r', '*g']\n",
    "for i, center in enumerate(centers):\n",
    "    plt.plot(center[0], center[1], mark[i], markersize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3c619f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector：\n",
      " [81 62]\n",
      "Score： 0.6888888888888889\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import model_selection\n",
    "from sklearn import svm\n",
    "# 加载数据集\n",
    "X = data  # 样本\n",
    "y = ground_truth_cluster  # 类别\n",
    "# 划分数据集\n",
    "X_trainer, X_tester, Y_trainer, Y_tester = model_selection.train_test_split(data, ground_truth_cluster, test_size=0.3)\n",
    "# 分类器\n",
    "# kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}, default='rbf'\n",
    "clf = svm.SVC(kernel=\"rbf\", probability=True)   # 参数kernel为线性核函数\n",
    "clf.fit(X_trainer, Y_trainer)  # 训练分类器\n",
    "print(\"Support Vector：\\n\", clf.n_support_)  # 每一类中属于支持向量的点数目\n",
    "# print(\"Predict：\\n\", clf.predict(X_test))  # 对测试集的预测结果\n",
    "score = clf.score(X_tester, Y_tester)  # 模型得分\n",
    "print(\"Score：\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "65d9b95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n",
      "100% |########################################################################|\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APFD:  0.5298314606741573 89\n",
      "RAUC:  0.6189431179775281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "SVM_Gini_act = []\n",
    "for data1 in data:\n",
    "    SVM_Gini_act.append(  clf.predict_proba(data1.reshape(1, -1))[0][0]  )\n",
    "\n",
    "indexs = np.argsort(SVM_Gini_act)\n",
    "indexs = indexs[::-1]\n",
    "# print(indexs)\n",
    "APFD,wrong_number,wrong_index = get_APFD(Gini_indexs=indexs, ground_truth_label=Y_test, \n",
    "                predicted_confidence=np.array(predicted_confidence))\n",
    "print(\"APFD: \", APFD, wrong_number)\n",
    "RAUC,_,_ = get_RAUC(Gini_indexs=indexs, ground_truth_label=Y_test, \n",
    "                predicted_confidence=np.array(predicted_confidence))\n",
    "print(\"RAUC: \", RAUC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6a72ce2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6538965324587667"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_proba(data1.reshape(1, -1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1d9848",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc247aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87e5312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f667b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac215a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "52bc65d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gini_act =[]\n",
    "pbar = ProgressBar()\n",
    "for i in pbar(range(len(grads_CE))):\n",
    "#     x_act = np.sum(np.abs(grads_CE[i]))\n",
    "    x_act =[np.sum(np.abs(grads_CE[i])),\n",
    "            np.sum(np.abs(grads_CE2[i])),\n",
    "#             np.sum(np.abs(grads_CE_min[i])),\n",
    "#             np.sum(np.abs(grads_RCE[i])),\n",
    "#             np.sum(np.abs(grads_CE_ave[i])),\n",
    "            np.sum(np.abs(grads_pre1[i])),\n",
    "            np.sum(np.abs(grads_pre2[i])),\n",
    "#             np.sum(np.abs(grads_pre_min[i]))\n",
    "           ]\n",
    "#     print(i,':',x_act)\n",
    "    Gini_act.append(x_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d6248789",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gini_act = np.array(Gini_act)\n",
    "indexs = np.argsort(Gini_act[:,0]*Gini_act[:,0])\n",
    "indexs = indexs[::-1]\n",
    "APFD,_,_ = get_APFD(Gini_indexs=indexs, ground_truth_label=Y_test, \n",
    "                predicted_confidence=np.array(predicted_confidence))\n",
    "print('APFD:', APFD)\n",
    "RAUC,_,_ = get_RAUC(Gini_indexs=indexs, ground_truth_label=Y_test, \n",
    "                predicted_confidence=np.array(predicted_confidence))\n",
    "print('RAUC:', RAUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9f1d7449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(grads_CE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8b8d24d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradients(img_input, model, top_pred_idx):\n",
    "    images = tf.cast(img_input, tf.float32)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(images)\n",
    "        preds = model(images)\n",
    "#         print(np.shape(images))\n",
    "#         print(np.shape(preds))\n",
    "        top_class = preds[:, top_pred_idx]\n",
    "\n",
    "    grads = tape.gradient(top_class, images)\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4726ff94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 784)               615440    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 720,378\n",
      "Trainable params: 720,378\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43982d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n",
      "100% |########################################################################|\n",
      "100% |########################################################################|\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APFD: 0.7498684210526315\n",
      "RAUC: 0.8690330477356181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# # use one layer's activation\n",
    "# get_activations = BE.function(inputs=model.inputs[0], outputs=model.layers[-1].output[:,:])\n",
    "# Gini_act = []\n",
    "# pbar = ProgressBar()\n",
    "# for x_tmp in pbar(X_test):    \n",
    "#     # 使用attention机制\n",
    "#     x_act = softmax(get_activations(x_tmp.reshape([-1,28*28])))\n",
    "#     att1 = np.dot(np.transpose(x_act), x_act)\n",
    "#     att_weight = np.sum(att1, axis = 1).reshape(np.shape(x_act))\n",
    "#     x_act_ = softmax(x_act*att_weight)\n",
    "#     Gini_tmp = 1-np.sum(x_act_*x_act_)\n",
    "#     Gini_tmp = -np.sum(x_act_*np.log2(x_act_))\n",
    "#     Gini_act.append(Gini_tmp)\n",
    "    \n",
    "#     # 未使用attention机制\n",
    "#     x_act = softmax(get_activations(x_tmp.reshape([-1,28*28])))\n",
    "#     Gini_tmp = 1-np.sum(x_act*x_act)\n",
    "# #     Gini_tmp = -np.sum(x_act*np.log2(x_act))\n",
    "#     Gini_act.append(Gini_tmp)\n",
    "\n",
    "# # use all layers' activation\n",
    "# pbar = ProgressBar()\n",
    "# get_activations1 = BE.function(inputs=model.inputs[0], outputs=model.layers[-1].output[:,:])\n",
    "# get_activations2 = BE.function(inputs=model.inputs[0], outputs=model.layers[-2].output[:,:])\n",
    "# get_activations3 = BE.function(inputs=model.inputs[0], outputs=model.layers[-3].output[:,:])\n",
    "# Gini_act = []\n",
    "# for x_tmp in pbar(X_test):\n",
    "#     x_act1 = softmax(get_activations1(x_tmp.reshape([-1,28*28])))\n",
    "#     x_act2 = softmax(get_activations2(x_tmp.reshape([-1,28*28])))\n",
    "#     x_act3 = softmax(get_activations3(x_tmp.reshape([-1,28*28])))\n",
    "# #     Gini_tmp = 1-np.sum(x_act3*x_act3)\n",
    "# #     Gini_tmp = 1-(np.sum(x_act1*x_act1)+np.sum(x_act2*x_act2)+np.sum(x_act3*x_act3))/3\n",
    "#     Gini_tmp = -np.sum(x_act3*np.log2(x_act3))\n",
    "# #     Gini_tmp = (-np.sum(x_act1*np.log2(x_act1))-np.sum(x_act2*np.log2(x_act2))-np.sum(x_act3*np.log2(x_act3)))/3\n",
    "#     Gini_act.append(Gini_tmp)\n",
    "    \n",
    "# indexs = np.argsort(Gini_act)\n",
    "# indexs = indexs[::-1]\n",
    "# o_i = 0\n",
    "# pbar = ProgressBar()\n",
    "# for i in pbar(range(0, (index1))):\n",
    "#     x_tmp = X_test[indexs[i]].reshape([-1,28*28])\n",
    "#     y_tmp = Y_test[indexs[i]]\n",
    "#     per_tmp = model.predict(x_tmp)\n",
    "#     if np.argmax(y_tmp)!=np.argmax(per_tmp):\n",
    "#         o_i = o_i+i\n",
    "# APFD = 1-o_i/(index1*(index1-index2))+1/(2*index1)\n",
    "# print(APFD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3854353e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ab7ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d156d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "APFD 指标基于基尼系数\n",
    " 使用所有层\n",
    "0.9357219241773963 使用倒数第三层\n",
    "0.946923104434907 使用倒数第二层\n",
    "0.921514556509299 使用倒数第一层，即最后一层（DeepGini）\n",
    "\n",
    "\n",
    "APFD 指标基于信息熵\n",
    "0.9608580114449213 使用所有层\n",
    "0.9608580114449213 使用倒数第三层\n",
    "0.9326023962804005 使用倒数第二层\n",
    "0.9212451001430615 使用倒数第一层，即最后一层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedbb8c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
