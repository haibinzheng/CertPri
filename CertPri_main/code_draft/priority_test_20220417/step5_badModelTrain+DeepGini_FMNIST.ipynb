{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90b61167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from progressbar import ProgressBar\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import backend as BE\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "config=tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True   #不全部占满显存, 按需分配\n",
    "sess=tf.compat.v1.Session(config=config)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66ca4a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(predict_label, ground_truth=None):\n",
    "    for i in predict_label[0]:\n",
    "        if i[0] == ground_truth:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def softmax( f ):\n",
    "    # instead: first shift the values of f so that the highest number is 0:\n",
    "    f -= np.max(f) # f becomes [-666, -333, 0]\n",
    "    return np.exp(f) / np.sum(np.exp(f))  # safe to do, gives the correct answer\n",
    "\n",
    "def get_APFD(Gini_indexs, ground_truth_label, predicted_confidence, top_set=None):\n",
    "    o_i = 0\n",
    "    pbar = ProgressBar()\n",
    "    wrong_num = 0\n",
    "    for i in pbar(range(0, len(Gini_indexs))):\n",
    "        if top_set is not None:\n",
    "            if not get_acc(predict_label=decode_predictions(predicted_confidence[Gini_indexs[i]], top=top_set), \n",
    "                           ground_truth=ground_truth_label[Gini_indexs[i]]):\n",
    "                o_i = o_i+i\n",
    "#                 print(i, o_i)\n",
    "                wrong_num = wrong_num+1\n",
    "        else:\n",
    "            if np.argmax(ground_truth_label[Gini_indexs[i]]) != np.argmax(predicted_confidence[Gini_indexs[i]]):\n",
    "                o_i = o_i+i\n",
    "                wrong_num = wrong_num+1\n",
    "    APFD = 1 - o_i/(len(Gini_indexs)*wrong_num) + 1/(2*len(Gini_indexs))\n",
    "    return APFD, len(Gini_indexs), wrong_num\n",
    "\n",
    "def get_RAUC(Gini_indexs, ground_truth_label, predicted_confidence, top_set=None):\n",
    "    pre_y_axis = []\n",
    "    o_i = 0\n",
    "    wrong_num = 0\n",
    "    pbar = ProgressBar()\n",
    "    for i in pbar(range(0, len(Gini_indexs))):\n",
    "        if top_set is not None:\n",
    "            if not get_acc(predict_label=decode_predictions(predicted_confidence[Gini_indexs[i]], top=top_set), \n",
    "                           ground_truth=ground_truth_label[Gini_indexs[i]]):  \n",
    "                o_i = o_i+1\n",
    "                wrong_num = wrong_num+1\n",
    "                pre_y_axis.append(o_i)\n",
    "            else:\n",
    "                pre_y_axis.append(o_i)\n",
    "        else:\n",
    "            if np.argmax(ground_truth_label[Gini_indexs[i]]) != np.argmax(predicted_confidence[Gini_indexs[i]]):\n",
    "                o_i = o_i+1\n",
    "                wrong_num = wrong_num+1\n",
    "                pre_y_axis.append(o_i)\n",
    "            else:\n",
    "                pre_y_axis.append(o_i)\n",
    "    true_y_axis = wrong_num*(len(Gini_indexs)-wrong_num) + (wrong_num+1)*wrong_num/2\n",
    "    RAUC = np.sum(pre_y_axis)/true_y_axis\n",
    "#     print(\"RAUC: \", RAUC)\n",
    "    return RAUC, len(Gini_indexs), wrong_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "992b6412",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()\n",
    "X_train = np.reshape(X_train,[-1,28*28])\n",
    "X_test = np.reshape(X_test,[-1,28*28])\n",
    "Y_train = to_categorical(Y_train,10)\n",
    "Y_test = to_categorical(Y_test,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc88dd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = 28*28\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(input_shape=(input_shape,), units=input_shape, activation='relu'))\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # model training\n",
    "# model.fit(X_train, Y_train, epochs=1, batch_size=512)\n",
    "\n",
    "# MODEL_PATH = \"/public/liujiawei/huawei/ZHB/ADF-master/models/\"\n",
    "# model.save(MODEL_PATH+\"fmnist_FC4_bad.h5\")\n",
    "\n",
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85833ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"/public/liujiawei/huawei/ZHB/ADF-master/models/\"\n",
    "model = load_model(MODEL_PATH+\"fmnist_FC4_bad.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c65d88ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 2s 2ms/step - loss: 1.2974 - accuracy: 0.7201\n",
      "accuracy 0.7200999855995178\n"
     ]
    }
   ],
   "source": [
    "loss_acc = model.evaluate(X_test, Y_test, batch_size=128)\n",
    "print('accuracy', loss_acc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9252c53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n",
      "100% |########################################################################|\n",
      "100% |########################################################################|\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APFD: 0.7485526315789472\n",
      "RAUC: 0.8675030599755202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "img_num = 200\n",
    "X_test = X_test[0:img_num]\n",
    "Y_test = Y_test[0:img_num]\n",
    "Gini = []\n",
    "predicted_confidence = []\n",
    "index1 = 0\n",
    "index2 = 0\n",
    "pbar = ProgressBar()\n",
    "for x_tmp in pbar(X_test):\n",
    "    per_tmp = model.predict(x_tmp.reshape([-1,28*28]))\n",
    "    label_tmp = np.argmax(per_tmp)\n",
    "    if label_tmp == np.argmax(Y_test[index1]):\n",
    "        index2 = index2 +1\n",
    "    index1 = index1 + 1\n",
    "    Gini_tmp = 1-np.sum(per_tmp*per_tmp)\n",
    "#     Gini_tmp = -np.sum(per_tmp*np.log2(per_tmp))\n",
    "    Gini.append(Gini_tmp)\n",
    "    predicted_confidence.append(per_tmp)\n",
    "    \n",
    "indexs = np.argsort(Gini)\n",
    "indexs = indexs[::-1]\n",
    "\n",
    "APFD,_,_ = get_APFD(Gini_indexs=indexs, ground_truth_label=Y_test, \n",
    "                predicted_confidence=np.array(predicted_confidence))\n",
    "print('APFD:', APFD)\n",
    "RAUC,_,_ = get_RAUC(Gini_indexs=indexs, ground_truth_label=Y_test, \n",
    "                predicted_confidence=np.array(predicted_confidence))\n",
    "print('RAUC:', RAUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "029ebacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test number: 200 \t misclassified number: 57\n"
     ]
    }
   ],
   "source": [
    "print('test number:', index1, '\\t misclassified number:', index1-index2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7497d0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# o_i = 0\n",
    "# for i in range(0, (index1)):\n",
    "#     x_tmp = X_test[indexs[i]].reshape([-1,28*28])\n",
    "#     y_tmp = Y_test[indexs[i]]\n",
    "#     per_tmp = model.predict(x_tmp)\n",
    "#     if np.argmax(y_tmp)!=np.argmax(per_tmp):\n",
    "#         o_i = o_i+i\n",
    "# APFD = 1-o_i/(index1*(index1-index2))+1/(2*index1)\n",
    "# print(APFD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a5b890e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 784)               615440    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 720,378\n",
      "Trainable params: 720,378\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b391129",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/public/liujiawei/anaconda3/envs/ZHB_env/lib/python3.6/site-packages/ipykernel_launcher.py:18: RuntimeWarning: divide by zero encountered in log2\n",
      "/public/liujiawei/anaconda3/envs/ZHB_env/lib/python3.6/site-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in multiply\n",
      "100% |########################################################################|\n",
      "100% |########################################################################|\n",
      "100% |########################################################################|\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APFD: (0.5150438596491227, 200, 57)\n",
      "RAUC: 0.5959812321501428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# use one layer's activation\n",
    "get_activations = BE.function(inputs=model.inputs[0], outputs=model.layers[-3].output[:,:])\n",
    "Gini_act = []\n",
    "pbar = ProgressBar()\n",
    "for x_tmp in pbar(X_test):    \n",
    "#     # 使用attention机制\n",
    "#     x_act = softmax(get_activations(x_tmp.reshape([-1,28*28])))\n",
    "#     att1 = np.dot(np.transpose(x_act), x_act)\n",
    "#     att_weight = np.sum(att1, axis = 1).reshape(np.shape(x_act))\n",
    "#     x_act_ = softmax(x_act*att_weight)\n",
    "#     Gini_tmp = 1-np.sum(x_act_*x_act_)\n",
    "#     Gini_tmp = -np.sum(x_act_*np.log2(x_act_))\n",
    "#     Gini_act.append(Gini_tmp)\n",
    "    \n",
    "    # 未使用attention机制\n",
    "    x_act = softmax(get_activations(x_tmp.reshape([-1,28*28])))\n",
    "#     Gini_tmp = 1-np.sum(x_act*x_act)\n",
    "    Gini_tmp = -np.sum(x_act*np.log2(x_act))\n",
    "    Gini_act.append(Gini_tmp)\n",
    "\n",
    "# # use all layers' activation\n",
    "# pbar = ProgressBar()\n",
    "# get_activations1 = BE.function(inputs=model.inputs[0], outputs=model.layers[-1].output[:,:])\n",
    "# get_activations2 = BE.function(inputs=model.inputs[0], outputs=model.layers[-2].output[:,:])\n",
    "# get_activations3 = BE.function(inputs=model.inputs[0], outputs=model.layers[-3].output[:,:])\n",
    "# Gini_act = []\n",
    "# for x_tmp in pbar(X_test):\n",
    "#     x_act1 = softmax(get_activations1(x_tmp.reshape([-1,28*28])))\n",
    "#     x_act2 = softmax(get_activations2(x_tmp.reshape([-1,28*28])))\n",
    "#     x_act3 = softmax(get_activations3(x_tmp.reshape([-1,28*28])))\n",
    "# #     Gini_tmp = 1-np.sum(x_act3*x_act3)\n",
    "# #     Gini_tmp = 1-(np.sum(x_act1*x_act1)+np.sum(x_act2*x_act2)+np.sum(x_act3*x_act3))/3\n",
    "#     Gini_tmp = -np.sum(x_act3*np.log2(x_act3))\n",
    "# #     Gini_tmp = (-np.sum(x_act1*np.log2(x_act1))-np.sum(x_act2*np.log2(x_act2))-np.sum(x_act3*np.log2(x_act3)))/3\n",
    "#     Gini_act.append(Gini_tmp)\n",
    "    \n",
    "indexs = np.argsort(Gini_act)\n",
    "indexs = indexs[::-1]\n",
    "APFD = get_APFD(Gini_indexs=indexs, ground_truth_label=Y_test, \n",
    "                predicted_confidence=np.array(predicted_confidence))\n",
    "print('APFD:', APFD)\n",
    "RAUC,_,_ = get_RAUC(Gini_indexs=indexs, ground_truth_label=Y_test, \n",
    "                predicted_confidence=np.array(predicted_confidence))\n",
    "print('RAUC:', RAUC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7415bc8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518bd744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bc65d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43982d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n",
      "100% |########################################################################|\n",
      "100% |########################################################################|\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APFD: 0.7498684210526315\n",
      "RAUC: 0.8690330477356181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# # use one layer's activation\n",
    "# get_activations = BE.function(inputs=model.inputs[0], outputs=model.layers[-1].output[:,:])\n",
    "# Gini_act = []\n",
    "# pbar = ProgressBar()\n",
    "# for x_tmp in pbar(X_test):    \n",
    "#     # 使用attention机制\n",
    "#     x_act = softmax(get_activations(x_tmp.reshape([-1,28*28])))\n",
    "#     att1 = np.dot(np.transpose(x_act), x_act)\n",
    "#     att_weight = np.sum(att1, axis = 1).reshape(np.shape(x_act))\n",
    "#     x_act_ = softmax(x_act*att_weight)\n",
    "#     Gini_tmp = 1-np.sum(x_act_*x_act_)\n",
    "#     Gini_tmp = -np.sum(x_act_*np.log2(x_act_))\n",
    "#     Gini_act.append(Gini_tmp)\n",
    "    \n",
    "#     # 未使用attention机制\n",
    "#     x_act = softmax(get_activations(x_tmp.reshape([-1,28*28])))\n",
    "#     Gini_tmp = 1-np.sum(x_act*x_act)\n",
    "# #     Gini_tmp = -np.sum(x_act*np.log2(x_act))\n",
    "#     Gini_act.append(Gini_tmp)\n",
    "\n",
    "# # use all layers' activation\n",
    "# pbar = ProgressBar()\n",
    "# get_activations1 = BE.function(inputs=model.inputs[0], outputs=model.layers[-1].output[:,:])\n",
    "# get_activations2 = BE.function(inputs=model.inputs[0], outputs=model.layers[-2].output[:,:])\n",
    "# get_activations3 = BE.function(inputs=model.inputs[0], outputs=model.layers[-3].output[:,:])\n",
    "# Gini_act = []\n",
    "# for x_tmp in pbar(X_test):\n",
    "#     x_act1 = softmax(get_activations1(x_tmp.reshape([-1,28*28])))\n",
    "#     x_act2 = softmax(get_activations2(x_tmp.reshape([-1,28*28])))\n",
    "#     x_act3 = softmax(get_activations3(x_tmp.reshape([-1,28*28])))\n",
    "# #     Gini_tmp = 1-np.sum(x_act3*x_act3)\n",
    "# #     Gini_tmp = 1-(np.sum(x_act1*x_act1)+np.sum(x_act2*x_act2)+np.sum(x_act3*x_act3))/3\n",
    "#     Gini_tmp = -np.sum(x_act3*np.log2(x_act3))\n",
    "# #     Gini_tmp = (-np.sum(x_act1*np.log2(x_act1))-np.sum(x_act2*np.log2(x_act2))-np.sum(x_act3*np.log2(x_act3)))/3\n",
    "#     Gini_act.append(Gini_tmp)\n",
    "    \n",
    "# indexs = np.argsort(Gini_act)\n",
    "# indexs = indexs[::-1]\n",
    "# o_i = 0\n",
    "# pbar = ProgressBar()\n",
    "# for i in pbar(range(0, (index1))):\n",
    "#     x_tmp = X_test[indexs[i]].reshape([-1,28*28])\n",
    "#     y_tmp = Y_test[indexs[i]]\n",
    "#     per_tmp = model.predict(x_tmp)\n",
    "#     if np.argmax(y_tmp)!=np.argmax(per_tmp):\n",
    "#         o_i = o_i+i\n",
    "# APFD = 1-o_i/(index1*(index1-index2))+1/(2*index1)\n",
    "# print(APFD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3854353e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ab7ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d156d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "APFD 指标基于基尼系数\n",
    " 使用所有层\n",
    "0.9357219241773963 使用倒数第三层\n",
    "0.946923104434907 使用倒数第二层\n",
    "0.921514556509299 使用倒数第一层，即最后一层（DeepGini）\n",
    "\n",
    "\n",
    "APFD 指标基于信息熵\n",
    "0.9608580114449213 使用所有层\n",
    "0.9608580114449213 使用倒数第三层\n",
    "0.9326023962804005 使用倒数第二层\n",
    "0.9212451001430615 使用倒数第一层，即最后一层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedbb8c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
