{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90b61167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from progressbar import ProgressBar\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tfv1\n",
    "from Integrated_Gradients_algorithm import *\n",
    "from GradVisualizer import *\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "# config=tf.ConfigProto()\n",
    "# config.gpu_options.allow_growth=True   #不全部占满显存, 按需分配\n",
    "# sess=tf.Session(config=config)\n",
    "config=tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True   #不全部占满显存, 按需分配\n",
    "sess=tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b5df65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "def get_loss_gradients(img_input, model, target_one_hot, from_logits=False):\n",
    "    images = tf.cast(img_input, tf.float32)\n",
    "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=from_logits)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(images)\n",
    "        preds = model(images)\n",
    "        loss = cce(target_one_hot, preds)\n",
    "#         top_class = preds[:, top_pred_idx]\n",
    "\n",
    "    grads = tape.gradient(loss, images)\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66ca4a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(predict_label, ground_truth=None):\n",
    "    for i in predict_label[0]:\n",
    "        if i[0] == ground_truth:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def softmax( f ):\n",
    "    # instead: first shift the values of f so that the highest number is 0:\n",
    "    f -= np.max(f) # f becomes [-666, -333, 0]\n",
    "    return np.exp(f) / np.sum(np.exp(f))  # safe to do, gives the correct answer\n",
    "\n",
    "def get_APFD(Gini_indexs, ground_truth_label, predicted_confidence, top_set=None):\n",
    "    o_i = 0\n",
    "    pbar = ProgressBar()\n",
    "    wrong_num = 0\n",
    "    for i in pbar(range(0, len(Gini_indexs))):\n",
    "        if top_set is not None:\n",
    "            if not get_acc(predict_label=decode_predictions(predicted_confidence[Gini_indexs[i]], top=top_set), \n",
    "                           ground_truth=ground_truth_label[Gini_indexs[i]]):\n",
    "                o_i = o_i+i\n",
    "#                 print(i, o_i)\n",
    "                wrong_num = wrong_num+1\n",
    "        else:\n",
    "            if np.argmax(ground_truth_label[Gini_indexs[i]]) != np.argmax(predicted_confidence[Gini_indexs[i]]):\n",
    "                o_i = o_i+i\n",
    "                wrong_num = wrong_num+1\n",
    "    APFD = 1 - o_i/(len(Gini_indexs)*wrong_num) + 1/(2*len(Gini_indexs))\n",
    "    return APFD, len(Gini_indexs), wrong_num\n",
    "\n",
    "def get_RAUC(Gini_indexs, ground_truth_label, predicted_confidence, top_set=None):\n",
    "    pre_y_axis = []\n",
    "    o_i = 0\n",
    "    wrong_num = 0\n",
    "    pbar = ProgressBar()\n",
    "    for i in pbar(range(0, len(Gini_indexs))):\n",
    "        if top_set is not None:\n",
    "            if not get_acc(predict_label=decode_predictions(predicted_confidence[Gini_indexs[i]], top=top_set), \n",
    "                           ground_truth=ground_truth_label[Gini_indexs[i]]):  \n",
    "                o_i = o_i+1\n",
    "                wrong_num = wrong_num+1\n",
    "                pre_y_axis.append(o_i)\n",
    "            else:\n",
    "                pre_y_axis.append(o_i)\n",
    "        else:\n",
    "            if np.argmax(ground_truth_label[Gini_indexs[i]]) != np.argmax(predicted_confidence[Gini_indexs[i]]):\n",
    "                o_i = o_i+1\n",
    "                wrong_num = wrong_num+1\n",
    "                pre_y_axis.append(o_i)\n",
    "            else:\n",
    "                pre_y_axis.append(o_i)\n",
    "    true_y_axis = wrong_num*(len(Gini_indexs)-wrong_num) + (wrong_num+1)*wrong_num/2\n",
    "    RAUC = np.sum(pre_y_axis)/true_y_axis\n",
    "#     print(\"RAUC: \", RAUC)\n",
    "    return RAUC, len(Gini_indexs), wrong_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "992b6412",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()\n",
    "X_train = np.reshape(X_train,[-1,28*28])\n",
    "X_test = np.reshape(X_test,[-1,28*28])\n",
    "Y_train = to_categorical(Y_train,10)\n",
    "Y_test = to_categorical(Y_test,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc88dd0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85833ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"/public/liujiawei/huawei/ZHB/ADF-master/models/\"\n",
    "model = load_model(MODEL_PATH+\"fmnist_FC5_good.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c65d88ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 2s 2ms/step - loss: 64.3066 - accuracy: 0.8529\n",
      "accuracy 0.8529000282287598\n"
     ]
    }
   ],
   "source": [
    "loss_acc = model.evaluate(X_test, Y_test, batch_size=128)\n",
    "print('accuracy', loss_acc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c6afeea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_29 (Dense)             (None, 784)               615440    \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 64)                65600     \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 1,608,922\n",
      "Trainable params: 1,608,922\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c56ba59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1df1c78",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_gradients() missing 1 required positional argument: 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c198615f48df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#     label_tmp = np.argmax(per_tmp)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#     print('label:', label_tmp, '\\t label confidence:', per_tmp[0,label_tmp])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_GE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_pred_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;31m#     grads = BE.gradients(loss = model.layers[-1].output[:,label_i], variables = model.inputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#     get_gradients = BE.function(inputs=model.inputs[0], outputs=grads)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: get_gradients() missing 1 required positional argument: 'model'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "max_iter = 1e+2\n",
    "class_num = 10\n",
    "decay_factor_gradent = 0.1\n",
    "decay_factor_epsilon_lr = 0.99\n",
    "# input_tensors = BE.placeholder(shape=(None, 28*28))\n",
    "# grads = BE.gradients(loss = model.total_loss, variables = model.inputs)\n",
    "x_GE_all = []\n",
    "\n",
    "for label_i in range(0,class_num):\n",
    "    x_GE = np.zeros((1, 28*28))\n",
    "#     per_tmp = model.predict(x_GE)\n",
    "#     label_tmp = np.argmax(per_tmp)\n",
    "#     print('label:', label_tmp, '\\t label confidence:', per_tmp[0,label_tmp])\n",
    "    grads = get_gradients(img_input=x_GE, top_pred_idx=label_i)\n",
    "#     grads = BE.gradients(loss = model.layers[-1].output[:,label_i], variables = model.inputs)\n",
    "#     get_gradients = BE.function(inputs=model.inputs[0], outputs=grads)\n",
    "#     x_moment = 0\n",
    "#     epsilon_lr = 5e-1\n",
    "#     for iter_i in range(0, int(max_iter)):\n",
    "#         x_grad = get_gradients(x_GE)[0]\n",
    "#         x_moment = x_moment * decay_factor_gradent + x_grad\n",
    "#         epsilon_lr = epsilon_lr * decay_factor_epsilon_lr\n",
    "#         x_GE = x_GE + x_moment * epsilon_lr\n",
    "#         x_GE = np.clip(x_GE, 0, 1)\n",
    "#         per_tmp = model.predict(x_GE)\n",
    "#         if 1-per_tmp[0, label_i]<1e-6 :\n",
    "#             break\n",
    "#     label_tmp = np.argmax(per_tmp)\n",
    "#     print('label:', label_tmp, '\\t label confidence:', per_tmp[0,label_tmp], '\\t iterations:',iter_i)\n",
    "#     x_GE_all.append(x_GE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e182ff96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grads = BE.gradients(loss = model.layers[-1].output[:,:], variables = model.inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89ae19f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83fe3b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e42eae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12947353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9252c53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n",
      "100% |########################################################################|\n",
      " 86% |#############################################################           |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APFD: 0.5119218558803534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAUC: 0.5524790673290377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "img_num = 10000\n",
    "X_test1 = X_test[0:img_num]\n",
    "Y_test1 = Y_test[0:img_num]\n",
    "Gini = []\n",
    "predicted_confidence = []\n",
    "pbar = ProgressBar()\n",
    "for x_tmp in pbar(X_test1):\n",
    "    per_tmp = model.predict(x_tmp.reshape([-1,28*28]))\n",
    "    Gini_tmp = 1-np.sum(per_tmp*per_tmp)\n",
    "#     Gini_tmp = -np.sum(per_tmp*np.log2(per_tmp))\n",
    "    Gini.append(Gini_tmp)\n",
    "    predicted_confidence.append(per_tmp)\n",
    "    \n",
    "indexs = np.argsort(Gini)\n",
    "indexs = indexs[::-1]\n",
    "\n",
    "APFD,_,wrong_number = get_APFD(Gini_indexs=indexs, ground_truth_label=Y_test, \n",
    "                predicted_confidence=np.array(predicted_confidence))\n",
    "print('APFD:', APFD)\n",
    "RAUC,_,_ = get_RAUC(Gini_indexs=indexs, ground_truth_label=Y_test, \n",
    "                predicted_confidence=np.array(predicted_confidence))\n",
    "print('RAUC:', RAUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "029ebacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test number: 500 \t misclassified number: 69\n"
     ]
    }
   ],
   "source": [
    "print('test number:', len(Gini), '\\t misclassified number:', wrong_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a5b890e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7497d0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 784)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5b391129",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n"
     ]
    }
   ],
   "source": [
    "# use one layer's activation\n",
    "# get_activations = BE.function(inputs=model.inputs[0], outputs=model.layers[-3].output[:,:])\n",
    "Gini_act = []\n",
    "grads_pre1 = []\n",
    "grads_pre2 = []\n",
    "grads_CE = []\n",
    "grads_CE2 = []\n",
    "num_class = 10\n",
    "# X_test1 = X_test[0:10]\n",
    "# Y_test1 = Y_test[0:10]\n",
    "pbar = ProgressBar()\n",
    "for x_tmp in pbar(X_test1):\n",
    "    x_tmp = x_tmp.reshape([-1,28*28])\n",
    "#     x_act = []\n",
    "#     for label_i in range(num_class):\n",
    "#         grads = get_gradients(img_input=x_tmp.reshape([-1,28*28]), model=model, top_pred_idx=label_i)\n",
    "#         x_act.append(np.sum(np.abs(grads)))\n",
    "#     x_act = np.array(x_act).reshape(-1, num_class)\n",
    "#     x_act = softmax(x_act)\n",
    "#     Gini_tmp = 1-np.sum(x_act*x_act)   \n",
    "#     Gini_act.append(Gini_tmp)\n",
    "    preds = model.predict(x_tmp)\n",
    "    \n",
    "    #     使用预测标签当做真实标签验证\n",
    "    label1 = np.argmax(preds[0])\n",
    "    grads = get_gradients(img_input=x_tmp, model=model, top_pred_idx=label1)\n",
    "    grads = np.sum(np.abs(grads))\n",
    "    grads_pre1.append(grads)\n",
    "    \n",
    "    target_one_hot = np.reshape(to_categorical(label1, num_class), (-1, num_class))\n",
    "    grads = get_loss_gradients(img_input=x_tmp, model=model, target_one_hot=target_one_hot)\n",
    "    grads = np.sum(np.abs(grads))\n",
    "    grads_CE.append(grads)\n",
    "    \n",
    "    preds[0][label1] = 0\n",
    "    label2 = np.argmax(preds[0])\n",
    "    grads = get_gradients(img_input=x_tmp, model=model, top_pred_idx=label2)\n",
    "    grads = np.sum(np.abs(grads))\n",
    "    grads_pre2.append(grads)\n",
    "    \n",
    "    target_one_hot = np.reshape(to_categorical(label2, num_class), (-1, num_class))\n",
    "    grads = get_loss_gradients(img_input=x_tmp, model=model, target_one_hot=target_one_hot)\n",
    "    grads = np.sum(np.abs(grads))\n",
    "    grads_CE2.append(grads)\n",
    "    \n",
    "#     preds[0][label1] = 0\n",
    "#     label2 = np.argmax(preds[0])\n",
    "#     grads = get_gradients(img_input=x_tmp, model=model, top_pred_idx=label2)\n",
    "#     grads_pre2.append(grads)\n",
    "    \n",
    "# #     grads = get_integrated_gradients(img_input=x_tmp, model=model, top_pred_idx=tf.argmax(preds[0]),num_steps=10, img_size=(1,28*28))\n",
    "#     grads = get_gradients(img_input=x_tmp, model=model, top_pred_idx=tf.argmax(preds[0]))\n",
    "# #     grads_min = get_integrated_gradients(img_input=x_tmp, model=model, top_pred_idx=tf.argmin(preds[0]),num_steps=10)\n",
    "# #     x_act = np.sum(np.abs(grads)) * np.sum(np.abs(grads_min))\n",
    "#     x_act = np.sum(np.abs(grads))\n",
    "\n",
    "#     grads = get_gradients(img_input=x_tmp.reshape([-1,28*28]), model=model, top_pred_idx=label_i)\n",
    "#     x_act.append(np.sum(np.abs(grads)))\n",
    "#     x_act = np.array(x_act).reshape(-1, num_class)\n",
    "#     x_act = softmax(x_act)\n",
    "#     Gini_tmp = 1-np.sum(x_act*x_act)   \n",
    "#     Gini_act.append(x_act)\n",
    "        \n",
    "#     # 使用attention机制\n",
    "#     x_act = softmax(get_activations(x_tmp.reshape([-1,28*28])))\n",
    "#     att1 = np.dot(np.transpose(x_act), x_act)\n",
    "#     att_weight = np.sum(att1, axis = 1).reshape(np.shape(x_act))\n",
    "#     x_act_ = softmax(x_act*att_weight)\n",
    "#     Gini_tmp = 1-np.sum(x_act_*x_act_)\n",
    "#     Gini_tmp = -np.sum(x_act_*np.log2(x_act_))\n",
    "#     Gini_act.append(Gini_tmp)\n",
    "    \n",
    "#     # 未使用attention机制\n",
    "#     x_act = softmax(get_activations(x_tmp.reshape([-1,28*28])))\n",
    "# #     Gini_tmp = 1-np.sum(x_act*x_act)\n",
    "#     Gini_tmp = -np.sum(x_act*np.log2(x_act))\n",
    "#     Gini_act.append(Gini_tmp)\n",
    "\n",
    "# # use all layers' activation\n",
    "# pbar = ProgressBar()\n",
    "# get_activations1 = BE.function(inputs=model.inputs[0], outputs=model.layers[-1].output[:,:])\n",
    "# get_activations2 = BE.function(inputs=model.inputs[0], outputs=model.layers[-2].output[:,:])\n",
    "# get_activations3 = BE.function(inputs=model.inputs[0], outputs=model.layers[-3].output[:,:])\n",
    "# Gini_act = []\n",
    "# for x_tmp in pbar(X_test):\n",
    "#     x_act1 = softmax(get_activations1(x_tmp.reshape([-1,28*28])))\n",
    "#     x_act2 = softmax(get_activations2(x_tmp.reshape([-1,28*28])))\n",
    "#     x_act3 = softmax(get_activations3(x_tmp.reshape([-1,28*28])))\n",
    "# #     Gini_tmp = 1-np.sum(x_act3*x_act3)\n",
    "# #     Gini_tmp = 1-(np.sum(x_act1*x_act1)+np.sum(x_act2*x_act2)+np.sum(x_act3*x_act3))/3\n",
    "#     Gini_tmp = -np.sum(x_act3*np.log2(x_act3))\n",
    "# #     Gini_tmp = (-np.sum(x_act1*np.log2(x_act1))-np.sum(x_act2*np.log2(x_act2))-np.sum(x_act3*np.log2(x_act3)))/3\n",
    "#     Gini_act.append(Gini_tmp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ac23fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7415bc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n",
      "100% |########################################################################|\n",
      "100% |########################################################################|\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APFD: 0.5743930319510536\n",
      "RAUC: 0.619906132704861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Gini_act =[]\n",
    "pbar = ProgressBar()\n",
    "for i in pbar(range(len(grads_CE))):\n",
    "#     x_act = np.sum(np.abs(grads_CE[i]))\n",
    "    x_act =[np.sum(np.abs(grads_CE[i])),\n",
    "            np.sum(np.abs(grads_CE2[i])),\n",
    "#             np.sum(np.abs(grads_CE_min[i])),\n",
    "#             np.sum(np.abs(grads_RCE[i])),\n",
    "#             np.sum(np.abs(grads_CE_ave[i])),\n",
    "            np.sum(np.abs(grads_pre1[i])),\n",
    "            np.sum(np.abs(grads_pre2[i])),\n",
    "#             np.sum(np.abs(grads_pre_min[i]))\n",
    "           ]\n",
    "#     print(i,':',x_act)\n",
    "    Gini_act.append(x_act)\n",
    "    \n",
    "Gini_act = np.array(Gini_act)\n",
    "indexs = np.argsort(np.abs(Gini_act[:,0]/(np.abs(Gini_act[:,1]))))\n",
    "indexs = indexs[::-1]\n",
    "APFD,_,_ = get_APFD(Gini_indexs=indexs, ground_truth_label=Y_test1, \n",
    "                predicted_confidence=np.array(predicted_confidence))\n",
    "print('APFD:', APFD)\n",
    "RAUC,_,_ = get_RAUC(Gini_indexs=indexs, ground_truth_label=Y_test1, \n",
    "                predicted_confidence=np.array(predicted_confidence))\n",
    "print('RAUC:', RAUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "518bd744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int64, numpy=1>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argmax(preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52bc65d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[199]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3854353e",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_integrated_gradients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ab7ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedbb8c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
