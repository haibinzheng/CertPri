{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90b61167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from progressbar import ProgressBar\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import backend as BE\n",
    "from tensorflow.keras.models import Model\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from Integrated_Gradients_algorithm import *\n",
    "\n",
    "# tf.enable_eager_execution(\n",
    "#     config=None,\n",
    "#     device_policy=None,\n",
    "#     execution_mode=None\n",
    "# )\n",
    "# tf.compat.v1.disable_eager_execution()\n",
    "# tf.config.run_functions_eagerly(True)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "config=tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True   #不全部占满显存, 按需分配\n",
    "sess=tf.compat.v1.Session(config=config)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "686692ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras\n",
    "def get_loss_gradients(img_input, model, target_one_hot, from_logits=False):\n",
    "    images = tf.cast(img_input, tf.float32)\n",
    "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=from_logits)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(images)\n",
    "        preds = model(images)\n",
    "        loss = cce(target_one_hot, preds)\n",
    "#         top_class = preds[:, top_pred_idx]\n",
    "\n",
    "    grads = tape.gradient(loss, images)\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66ca4a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(predict_label, ground_truth=None):\n",
    "    for i in predict_label[0]:\n",
    "        if i[0] == ground_truth:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def softmax( f ):\n",
    "    # instead: first shift the values of f so that the highest number is 0:\n",
    "    s = f-np.max(f) # f becomes [-666, -333, 0]\n",
    "    return np.exp(s) / np.sum(np.exp(s))  # safe to do, gives the correct answer\n",
    "\n",
    "def get_APFD(Gini_indexs, ground_truth_label, predicted_confidence, top_set=None):\n",
    "    o_i = 0\n",
    "    pbar = ProgressBar()\n",
    "    wrong_num = 0\n",
    "    wrong_index = []\n",
    "    for i in pbar(range(0, len(Gini_indexs))):\n",
    "        if top_set is not None:\n",
    "            if not get_acc(predict_label=decode_predictions(predicted_confidence[Gini_indexs[i]], top=top_set), \n",
    "                           ground_truth=ground_truth_label[Gini_indexs[i]]):\n",
    "                o_i = o_i+i\n",
    "#                 print(i, o_i)\n",
    "                wrong_num = wrong_num+1\n",
    "                wrong_index.append(i)\n",
    "        else:\n",
    "            if np.argmax(ground_truth_label[Gini_indexs[i]]) != np.argmax(predicted_confidence[Gini_indexs[i]]):\n",
    "                o_i = o_i+i\n",
    "                wrong_num = wrong_num+1\n",
    "                wrong_index.append(i)\n",
    "    APFD = 1 - o_i/(len(Gini_indexs)*wrong_num) + 1/(2*len(Gini_indexs))\n",
    "    return APFD, wrong_num, wrong_index\n",
    "\n",
    "def get_RAUC(Gini_indexs, ground_truth_label, predicted_confidence, top_set=None):\n",
    "    pre_y_axis = []\n",
    "    o_i = 0\n",
    "    wrong_num = 0\n",
    "    pbar = ProgressBar()\n",
    "    for i in pbar(range(0, len(Gini_indexs))):\n",
    "        if top_set is not None:\n",
    "            if not get_acc(predict_label=decode_predictions(predicted_confidence[Gini_indexs[i]], top=top_set), \n",
    "                           ground_truth=ground_truth_label[Gini_indexs[i]]):  \n",
    "                o_i = o_i+1\n",
    "                wrong_num = wrong_num+1\n",
    "                pre_y_axis.append(o_i)\n",
    "            else:\n",
    "                pre_y_axis.append(o_i)\n",
    "        else:\n",
    "            if np.argmax(ground_truth_label[Gini_indexs[i]]) != np.argmax(predicted_confidence[Gini_indexs[i]]):\n",
    "                o_i = o_i+1\n",
    "                wrong_num = wrong_num+1\n",
    "                pre_y_axis.append(o_i)\n",
    "            else:\n",
    "                pre_y_axis.append(o_i)\n",
    "    true_y_axis = wrong_num*(len(Gini_indexs)-wrong_num) + (wrong_num+1)*wrong_num/2\n",
    "    RAUC = np.sum(pre_y_axis)/true_y_axis\n",
    "#     print(\"RAUC: \", RAUC)\n",
    "    return RAUC, len(Gini_indexs), wrong_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cc8fa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "992b6412",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()\n",
    "X_train = np.reshape(X_train,[-1,28*28])\n",
    "X_test = np.reshape(X_test,[-1,28*28])\n",
    "Y_train = to_categorical(Y_train,10)\n",
    "Y_test = to_categorical(Y_test,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc88dd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_shape = 28*28\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Dense(input_shape=(input_shape,), units=input_shape, activation='relu'))\n",
    "# model.add(Dense(units=128, activation='relu'))\n",
    "# model.add(Dense(units=32, activation='relu'))\n",
    "# model.add(Dense(units=10, activation='softmax'))\n",
    "# # model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # # model training\n",
    "# # model.fit(X_train, Y_train, epochs=1, batch_size=512)\n",
    "\n",
    "# # MODEL_PATH = \"/public/liujiawei/huawei/ZHB/ADF-master/models/\"\n",
    "# # model.save(MODEL_PATH+\"fmnist_FC4_bad.h5\")\n",
    "\n",
    "# # del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85833ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"/public/liujiawei/huawei/ZHB/ADF-master/models/\"\n",
    "model = load_model(MODEL_PATH+\"fmnist_FC4_bad.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c65d88ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 1s 2ms/step - loss: 1.2974 - accuracy: 0.7201\n",
      "accuracy 0.7200999855995178\n"
     ]
    }
   ],
   "source": [
    "loss_acc = model.evaluate(X_test, Y_test, batch_size=128)\n",
    "print('accuracy', loss_acc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9252c53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n",
      "100% |########################################################################|\n",
      "100% |########################################################################|\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APFD: 0.745625\n",
      "RAUC: 0.8764792899408284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "img_num = 100\n",
    "X_test = X_test[0:img_num]\n",
    "Y_test = Y_test[0:img_num]\n",
    "Gini = []\n",
    "predicted_confidence = []\n",
    "index1 = 0\n",
    "index2 = 0\n",
    "pbar = ProgressBar()\n",
    "for x_tmp in pbar(X_test):\n",
    "    per_tmp = model.predict(x_tmp.reshape([-1,28*28]))\n",
    "    label_tmp = np.argmax(per_tmp)\n",
    "    if label_tmp == np.argmax(Y_test[index1]):\n",
    "        index2 = index2 +1\n",
    "    index1 = index1 + 1\n",
    "    Gini_tmp = 1-np.sum(per_tmp*per_tmp)\n",
    "#     Gini_tmp = -np.sum(per_tmp*np.log2(per_tmp))\n",
    "    Gini.append(Gini_tmp)\n",
    "    predicted_confidence.append(per_tmp)\n",
    "    \n",
    "indexs = np.argsort(Gini)\n",
    "indexs = indexs[::-1]\n",
    "\n",
    "APFD,_,_ = get_APFD(Gini_indexs=indexs, ground_truth_label=Y_test, \n",
    "                predicted_confidence=np.array(predicted_confidence))\n",
    "print('APFD:', APFD)\n",
    "RAUC,_,_ = get_RAUC(Gini_indexs=indexs, ground_truth_label=Y_test, \n",
    "                predicted_confidence=np.array(predicted_confidence))\n",
    "print('RAUC:', RAUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "029ebacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test number: 1000 \t misclassified number: 285\n"
     ]
    }
   ],
   "source": [
    "print('test number:', index1, '\\t misclassified number:', index1-index2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7497d0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# o_i = 0\n",
    "# for i in range(0, (index1)):\n",
    "#     x_tmp = X_test[indexs[i]].reshape([-1,28*28])\n",
    "#     y_tmp = Y_test[indexs[i]]\n",
    "#     per_tmp = model.predict(x_tmp)\n",
    "#     if np.argmax(y_tmp)!=np.argmax(per_tmp):\n",
    "#         o_i = o_i+i\n",
    "# APFD = 1-o_i/(index1*(index1-index2))+1/(2*index1)\n",
    "# print(APFD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a5b890e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 784)               615440    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 720,378\n",
      "Trainable params: 720,378\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b391129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use one layer's activation\n",
    "# get_activations = BE.function(inputs=model.inputs[0], outputs=model.layers[-3].output[:,:])\n",
    "# Gini_act = []\n",
    "# pbar = ProgressBar()\n",
    "# for x_tmp in pbar(X_test):    \n",
    "# #     # 使用attention机制\n",
    "# #     x_act = softmax(get_activations(x_tmp.reshape([-1,28*28])))\n",
    "# #     att1 = np.dot(np.transpose(x_act), x_act)\n",
    "# #     att_weight = np.sum(att1, axis = 1).reshape(np.shape(x_act))\n",
    "# #     x_act_ = softmax(x_act*att_weight)\n",
    "# #     Gini_tmp = 1-np.sum(x_act_*x_act_)\n",
    "# #     Gini_tmp = -np.sum(x_act_*np.log2(x_act_))\n",
    "# #     Gini_act.append(Gini_tmp)\n",
    "    \n",
    "#     # 未使用attention机制\n",
    "#     x_act = softmax(get_activations(x_tmp.reshape([-1,28*28])))\n",
    "# #     Gini_tmp = 1-np.sum(x_act*x_act)\n",
    "#     Gini_tmp = -np.sum(x_act*np.log2(x_act))\n",
    "#     Gini_act.append(Gini_tmp)\n",
    "\n",
    "# # # use all layers' activation\n",
    "# # pbar = ProgressBar()\n",
    "# # get_activations1 = BE.function(inputs=model.inputs[0], outputs=model.layers[-1].output[:,:])\n",
    "# # get_activations2 = BE.function(inputs=model.inputs[0], outputs=model.layers[-2].output[:,:])\n",
    "# # get_activations3 = BE.function(inputs=model.inputs[0], outputs=model.layers[-3].output[:,:])\n",
    "# # Gini_act = []\n",
    "# # for x_tmp in pbar(X_test):\n",
    "# #     x_act1 = softmax(get_activations1(x_tmp.reshape([-1,28*28])))\n",
    "# #     x_act2 = softmax(get_activations2(x_tmp.reshape([-1,28*28])))\n",
    "# #     x_act3 = softmax(get_activations3(x_tmp.reshape([-1,28*28])))\n",
    "# # #     Gini_tmp = 1-np.sum(x_act3*x_act3)\n",
    "# # #     Gini_tmp = 1-(np.sum(x_act1*x_act1)+np.sum(x_act2*x_act2)+np.sum(x_act3*x_act3))/3\n",
    "# #     Gini_tmp = -np.sum(x_act3*np.log2(x_act3))\n",
    "# # #     Gini_tmp = (-np.sum(x_act1*np.log2(x_act1))-np.sum(x_act2*np.log2(x_act2))-np.sum(x_act3*np.log2(x_act3)))/3\n",
    "# #     Gini_act.append(Gini_tmp)\n",
    "    \n",
    "# indexs = np.argsort(Gini_act)\n",
    "# indexs = indexs[::-1]\n",
    "# APFD = get_APFD(Gini_indexs=indexs, ground_truth_label=Y_test, \n",
    "#                 predicted_confidence=np.array(predicted_confidence))\n",
    "# print('APFD:', APFD)\n",
    "# RAUC,_,_ = get_RAUC(Gini_indexs=indexs, ground_truth_label=Y_test, \n",
    "#                 predicted_confidence=np.array(predicted_confidence))\n",
    "# print('RAUC:', RAUC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ce5c0d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     输出层对模型的某一层求导\n",
    "def get_hidden_layer_gradient(x_input, model, pre_conf, layers_names):\n",
    "    top_pred_idx = np.argmax(pre_conf)\n",
    "    hidden_layer = model.get_layer(layers_names).output\n",
    "    grads = BE.gradients(loss = model.layers[-1].output[:, top_pred_idx], variables = hidden_layer)\n",
    "    get_gradients = BE.function(inputs=model.inputs[0], outputs=grads)\n",
    "    layer_grad = get_gradients(x_input)[0]\n",
    "    return layer_grad\n",
    "    \n",
    "#     损失函数对模型的某一层求导\n",
    "def get_hidden_layer_loss_gradient(x_input, model, pre_conf, layers_names):\n",
    "    top_pred_idx = np.argmax(pre_conf)\n",
    "    num_class = np.shape(pre_conf)[-1]\n",
    "    hidden_layer = model.get_layer(layers_names).output\n",
    "    loss = BE.categorical_crossentropy(to_categorical(top_pred_idx, num_class), model.layers[-1].output[:,:][0])\n",
    "#     loss = BE.categorical_crossentropy(to_categorical(label_tmp, num_class), pre_conf[0]) # 用 preds[0]会报NoneType错\n",
    "    grads = BE.gradients(loss = loss, variables = hidden_layer)\n",
    "    get_gradients = BE.function(inputs=model.inputs[0], outputs=grads)\n",
    "    layer_grad = get_gradients(x_input)[0]\n",
    "    return layer_grad\n",
    "\n",
    "def GradientEstimator(samples, sigma, model, x, bounds, noise_mu, nise_std, clip=True):\n",
    "#     value = loss_fn(x)\n",
    "    gradient = np.zeros_like(x)\n",
    "    bounds_lower, bounds_upper = bounds\n",
    "    for k in range(samples // 2):\n",
    "        noise = np.random.normal(noise_mu, nise_std, x.shape)\n",
    "\n",
    "        pos_theta = x + sigma * noise\n",
    "        neg_theta = x - sigma * noise\n",
    "\n",
    "        if clip:\n",
    "            pos_theta = pos_theta.clip(bounds_lower, bounds_upper)\n",
    "            neg_theta = neg_theta.clip(bounds_lower, bounds_upper)\n",
    "\n",
    "        pos_preds = model.predict(pos_theta)\n",
    "        pos_loss = BE.categorical_crossentropy( to_categorical(np.argmax(pos_preds), len(pos_preds[0])), pos_preds[0] )\n",
    "        neg_preds = model.predict(neg_theta)\n",
    "        neg_loss = BE.categorical_crossentropy( to_categorical(np.argmax(neg_preds), len(neg_preds[0])), neg_preds[0] )\n",
    "#         pos_loss = loss_fn(pos_theta)\n",
    "#         neg_loss = loss_fn(neg_theta)\n",
    "        gradient += (pos_loss - neg_loss) * noise\n",
    "    gradient /= 2 * sigma * 2 * samples\n",
    "    return gradient\n",
    "\n",
    "def GradientEstimator_layer(samples, sigma, model, x, bounds, noise_mu, nise_std, layer_name, clip=True):\n",
    "#     value = loss_fn(x)\n",
    "    model_hidden_layer = Model(inputs=model.input, outputs=model.get_layer(layers_names).output)\n",
    "    gradient = np.zeros_like(model_hidden_layer)\n",
    "    bounds_lower, bounds_upper = bounds\n",
    "    for k in range(samples // 2):\n",
    "        noise = np.random.normal(noise_mu, nise_std, x.shape)\n",
    "\n",
    "        pos_theta = x + sigma * noise\n",
    "        neg_theta = x - sigma * noise\n",
    "\n",
    "        if clip:\n",
    "            pos_theta = pos_theta.clip(bounds_lower, bounds_upper)\n",
    "            neg_theta = neg_theta.clip(bounds_lower, bounds_upper)\n",
    "\n",
    "        pos_preds = model.predict(pos_theta)\n",
    "        pos_features = model_hidden_layer.predict(pos_theta)\n",
    "        pos_loss = BE.categorical_crossentropy( to_categorical(np.argmax(pos_preds), len(pos_preds[0])), pos_preds[0] )\n",
    "        \n",
    "        neg_preds = model.predict(neg_theta)\n",
    "        neg_features = model_hidden_layer.predict(neg_theta)\n",
    "        neg_loss = BE.categorical_crossentropy( to_categorical(np.argmax(neg_preds), len(neg_preds[0])), neg_preds[0] )\n",
    "        \n",
    "        noise_feature = (pos_features-neg_features)/(2*sigma)\n",
    "#         pos_loss = loss_fn(pos_theta)\n",
    "#         neg_loss = loss_fn(neg_theta)\n",
    "        gradient += (pos_loss - neg_loss) * noise_feature\n",
    "    gradient /= 2 * sigma * 2 * samples\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2445d11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7415bc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n"
     ]
    }
   ],
   "source": [
    "# use one layer's activation\n",
    "pbar = ProgressBar()\n",
    "# layer_pos = -2\n",
    "# get_activations = BE.function(inputs=model.inputs[0], outputs=model.layers[layer_pos].output[:,:])\n",
    "grads_pre1 = []\n",
    "grads_pre2 = []\n",
    "grads_CE = []\n",
    "grads_CE2 = []\n",
    "\n",
    "grads_Out_HL = []\n",
    "grads_Loss_HL = []\n",
    "feature_HL = []\n",
    "grads_Estor = []\n",
    "\n",
    "num_class = 10\n",
    "# predicted_confidence = []\n",
    "# ground_truth_label = []\n",
    "for x_tmp in pbar(X_test):\n",
    "    x_tmp = x_tmp.reshape(1, -1)\n",
    "    preds = model.predict(x_tmp)\n",
    "    label_tmp = np.argmax(preds)\n",
    "    layers_names = 'dense_2'\n",
    "    \n",
    "#     #     使用对输出的梯度估计\n",
    "#     label1 = np.argmax(preds[0])\n",
    "#     target_one_hot = np.reshape(to_categorical(label1, num_class), (-1, num_class))\n",
    "#     samples = 8 # 采样频率\n",
    "#     sigma = 2  # 扰动尺寸\n",
    "#     bounds = [np.min(x_tmp), np.max(x_tmp)]\n",
    "#     noise_mu = 0\n",
    "#     nise_std = 0.1\n",
    "#     grads = GradientEstimator(samples, sigma, model, x_tmp, bounds, noise_mu, nise_std)\n",
    "#     grads_Estor.append(grads)\n",
    "\n",
    "    #     使用损失对某一层的梯度估计\n",
    "    label1 = np.argmax(preds[0])\n",
    "    target_one_hot = np.reshape(to_categorical(label1, num_class), (-1, num_class))\n",
    "    samples = 14 # 采样频率\n",
    "    sigma = 2  # 扰动尺寸\n",
    "    bounds = [np.min(x_tmp), np.max(x_tmp)]\n",
    "    noise_mu = 0\n",
    "    nise_std = 10\n",
    "    grads = GradientEstimator_layer(samples, sigma, model, x_tmp, bounds, noise_mu, nise_std, layers_names)\n",
    "    grads_Estor.append(grads)\n",
    "\n",
    "# #     损失函数对模型的某一层求导\n",
    "#     layer_grads = get_hidden_layer_loss_gradient(x_input=x_tmp, model=model, pre_conf=preds, layers_names=layers_names)\n",
    "#     grads_CE.append(layer_grads)\n",
    "        \n",
    "# #     输出层对模型的某一层求导\n",
    "#     layer_grads = get_hidden_layer_gradient(x_input, model, pre_conf, layers_names)\n",
    "        \n",
    "        \n",
    "#     hidden_layer = model.get_layer(layers_names).output\n",
    "# #     cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "# #     with tf.GradientTape() as tape:\n",
    "# #         tape.watch(hidden_layer)\n",
    "# #         preds = model(x_tmp)\n",
    "# #         loss = cce(to_categorical(top_pred_idx, num_class), preds)\n",
    "# # #         top_class = preds[:, top_pred_idx]\n",
    "# #     grads = tape.gradient(top_class, tf.cast(x_tmp, tf.float32))\n",
    "# #     grads = BE.gradients(loss = model.layers[-1].output[:,label_tmp], variables = hidden_layer)\n",
    "# #     grads = BE.gradients(loss = cce(to_categorical(label_tmp, num_class), preds[0]), variables = hidden_layer)\n",
    "\n",
    "# #     损失函数对模型的某一层求导\n",
    "# #     model_hidden_layer = Model(inputs=model.input, outputs=model.get_layer(layers_names).output)\n",
    "#     hidden_layer = model.get_layer(layers_names).output\n",
    "#     loss = BE.categorical_crossentropy( to_categorical(label_tmp, num_class), model.layers[-1].output[:,:][0] )\n",
    "#     grads = BE.gradients(loss = loss, variables = hidden_layer)\n",
    "#     get_gradients_loss = BE.function(inputs=model.inputs[0], outputs=grads)\n",
    "#     x_grad = get_gradients_loss(x_tmp)[0]\n",
    "# #     features = model_hidden_layer.predict(x_tmp)\n",
    "#     grads_Loss_HL.append(x_grad)\n",
    "# #     feature_HL.append(features)\n",
    "    \n",
    "# #     输出层对模型的某一层求导\n",
    "#     hidden_layer = model.get_layer(layers_names).output\n",
    "#     grads = BE.gradients(loss = model.layers[-1].output[:,label_tmp], variables = hidden_layer)\n",
    "#     get_gradients_Out = BE.function(inputs=model.inputs[0], outputs=grads)\n",
    "#     x_grad = get_gradients_Out(x_tmp)[0]\n",
    "#     grads_Out_HL.append(x_grad)\n",
    "    \n",
    "#     target_one_hot = np.ones(num_class)*(1.0/num_class)\n",
    "#     grads = get_loss_gradients(img_input=x_tmp, model=model, target_one_hot=target_one_hot)\n",
    "#     grads = grads.numpy()\n",
    "#     grads = np.sum(np.abs(grads))\n",
    "#     grads_CE.append(grads)\n",
    "    \n",
    "#     # 计算每一类的梯度\n",
    "#     grads_sum = np.zeros(num_class)\n",
    "#     for label_i in range(num_class):\n",
    "# #         target_one_hot = np.reshape(to_categorical(label_i, num_class), (-1, num_class))\n",
    "# #         grads = get_loss_gradients(img_input=x_tmp, model=model, target_one_hot=target_one_hot)\n",
    "\n",
    "#         grads = get_gradients(img_input=x_tmp, model=model, top_pred_idx=label_i)\n",
    "# #         grads = grads.numpy()\n",
    "#         grads = np.sum(np.abs(grads))\n",
    "# #         grads = np.sum(grads)\n",
    "#         grads_sum[label_i] = grads*grads\n",
    "#     grads_CE.append(grads_sum)\n",
    "    \n",
    "#     #     使用预测标签当做真实标签验证\n",
    "#     label1 = np.argmax(preds[0])\n",
    "#     grads = get_gradients(img_input=x_tmp, model=model, top_pred_idx=label1)\n",
    "# #     grads = grads.numpy()\n",
    "# #     grads = BE.sum(BE.abs(grads))\n",
    "#     grads_pre1.append(grads)\n",
    "    \n",
    "#     target_one_hot = np.reshape(to_categorical(label1, num_class), (-1, num_class))\n",
    "#     grads = get_loss_gradients(img_input=x_tmp, model=model, target_one_hot=target_one_hot)\n",
    "# #     grads = grads.numpy()\n",
    "# #     grads = BE.sum(BE.abs(grads))\n",
    "#     grads_CE.append(grads)\n",
    "    \n",
    "#     preds[0][label1] = 0\n",
    "#     label2 = np.argmax(preds[0])\n",
    "#     grads = get_gradients(img_input=x_tmp, model=model, top_pred_idx=label2)\n",
    "# #     grads = grads.numpy()\n",
    "# #     grads = BE.sum(BE.abs(grads))\n",
    "#     grads_pre2.append(grads)\n",
    "    \n",
    "#     target_one_hot = np.reshape(to_categorical(label2, num_class), (-1, num_class))\n",
    "#     grads = get_loss_gradients(img_input=x_tmp, model=model, target_one_hot=target_one_hot)\n",
    "# #     grads = grads.numpy()\n",
    "# #     grads = BE.sum(BE.abs(grads))\n",
    "#     grads_CE2.append(grads)\n",
    "\n",
    "#     # 使用attention机制\n",
    "#     x_act = softmax(get_activations(x_tmp.reshape([-1,28*28])))\n",
    "#     att1 = np.dot(np.transpose(x_act), x_act)\n",
    "#     att_weight = np.sum(att1, axis = 1).reshape(np.shape(x_act))\n",
    "#     x_act_ = softmax(x_act*att_weight)\n",
    "#     Gini_tmp = 1-np.sum(x_act_*x_act_)\n",
    "#     Gini_act.append(Gini_tmp)\n",
    "    \n",
    "#     predicted_confidence.append(model.predict(x_tmp.reshape([-1,28*28])))\n",
    "#      # 未使用attention机制\n",
    "#     x_act = softmax(get_activations(x_tmp.reshape([-1,28*28])))\n",
    "#     Gini_tmp = 1-np.sum(x_act*x_act)\n",
    "#     Gini_act.append(Gini_tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5941c463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(x_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "88847a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n"
     ]
    }
   ],
   "source": [
    "# Gini_act = []\n",
    "# # grads_CE = np.array(grads_CE)\n",
    "# for i in range(len(grads_CE)):\n",
    "#     Gini_act.append(  1-np.sum(softmax(grads_CE[i]) * softmax(grads_CE[i])) )\n",
    "\n",
    "Gini_act =[]\n",
    "pbar = ProgressBar()\n",
    "for i in pbar(range(len(grads_Estor))):\n",
    "#     x_act = np.sum(np.abs(grads_CE[i]))\n",
    "    x_act = np.sum(grads_Estor[i]*grads_Estor[i])\n",
    "#     print(i,':',x_act)\n",
    "    Gini_act.append(x_act)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddcf974",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d8d0cea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Gini_act = np.array(Gini_act)\n",
    "# # Gini_act.shape\n",
    "# # print(np.max(Gini_act), np.max(grads_CE))\n",
    "# DATA_PATH = '/public/liujiawei/huawei/ZHB/ADF-master/datasets/'\n",
    "# np.save(DATA_PATH + \"FMNIST_hidden_gradient_1000samples.npy\", {\"grads_Loss_HL\":grads_Loss_HL, \n",
    "#                                                        \"feature_HL\":feature_HL,\n",
    "#                                                         \"grads_Out_HL\":grads_Out_HL})\n",
    "# # np.save(DATA_PATH + \"FMNIST_input_gradient_1000samples.npy\", {\"grads_pre1\":grads_pre1,\n",
    "# #                                                         \"grads_pre2\":grads_pre2,\n",
    "# #                                                         \"grads_CE\":grads_CE,\n",
    "# #                                                         \"grads_CE2\":grads_CE2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "78aeb139",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n",
      "100% |########################################################################|\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APFD: 0.6525\n",
      "RAUC: 0.7662721893491125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Gini_act = np.array(Gini_act)\n",
    "# indexs = np.argsort(Gini_act[:,3]*Gini_act[:,3])\n",
    "indexs = np.argsort(Gini_act)\n",
    "indexs = indexs[::-1]\n",
    "APFD,_,wrong_index = get_APFD(Gini_indexs=indexs, ground_truth_label=Y_test, \n",
    "                predicted_confidence=np.array(predicted_confidence))\n",
    "print('APFD:', APFD)\n",
    "RAUC,_,_ = get_RAUC(Gini_indexs=indexs, ground_truth_label=Y_test, \n",
    "                predicted_confidence=np.array(predicted_confidence))\n",
    "print('RAUC:', RAUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a45538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "aa31f940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(ground_truth_cluster)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans,AffinityPropagation, SpectralClustering\n",
    "\n",
    "# 读取数据\n",
    "data = np.squeeze(np.array(grads_Loss_HL), axis = 1)\n",
    "# 聚类数量\n",
    "k = 2\n",
    "# 训练模型\n",
    "model_kmeans = KMeans(n_clusters=k)\n",
    "model_kmeans.fit(data)\n",
    "# 分类中心点坐标\n",
    "centers = model_kmeans.cluster_centers_\n",
    "# 预测结果\n",
    "result = model_kmeans.predict(data)\n",
    "ground_truth_cluster = np.zeros([len(result)], dtype=np.int32)\n",
    "ground_truth_cluster[wrong_index]=1\n",
    "\n",
    "# 用不同的颜色绘制数据点\n",
    "mark = ['or', 'og']\n",
    "for i, d in enumerate(data):\n",
    "    plt.plot(d[0], d[1], mark[ground_truth_cluster[i]])\n",
    "# 画出各个分类的中心点\n",
    "mark = ['*b', '*y']\n",
    "for i, center in enumerate(centers):\n",
    "    plt.plot(center[0], center[1], mark[i], markersize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "048b3062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector：\n",
      " [225 204]\n",
      "Score： 0.73\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import model_selection\n",
    "from sklearn import svm\n",
    "# 加载数据集\n",
    "X = data  # 样本\n",
    "y = ground_truth_cluster  # 类别\n",
    "# 划分数据集\n",
    "X_trainer, X_tester, Y_trainer, Y_tester = model_selection.train_test_split(data, ground_truth_cluster, test_size=0.3)\n",
    "# 分类器\n",
    "# kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}, default='rbf'\n",
    "clf = svm.SVC(kernel=\"linear\", probability=True)   # 参数kernel为线性核函数\n",
    "clf.fit(X_trainer, Y_trainer)  # 训练分类器\n",
    "print(\"Support Vector：\\n\", clf.n_support_)  # 每一类中属于支持向量的点数目\n",
    "# print(\"Predict：\\n\", clf.predict(X_test))  # 对测试集的预测结果\n",
    "score = clf.score(X_tester, Y_tester)  # 模型得分\n",
    "print(\"Score：\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "388818e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n",
      "100% |########################################################################|\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APFD:  0.6348333333333334 285\n",
      "RAUC:  0.7393162393162394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "SVM_Gini_act = []\n",
    "for data1 in data:\n",
    "    SVM_Gini_act.append(  clf.predict_proba(data1.reshape(1, -1))[0][0]  )\n",
    "\n",
    "indexs = np.argsort(SVM_Gini_act)\n",
    "indexs = indexs[::-1]\n",
    "# print(indexs)\n",
    "APFD,wrong_number,wrong_index = get_APFD(Gini_indexs=indexs, ground_truth_label=Y_test, \n",
    "                predicted_confidence=np.array(predicted_confidence))\n",
    "print(\"APFD: \", APFD, wrong_number)\n",
    "RAUC,_,_ = get_RAUC(Gini_indexs=indexs, ground_truth_label=Y_test, \n",
    "                predicted_confidence=np.array(predicted_confidence))\n",
    "print(\"RAUC: \", RAUC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d26456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "e0023710",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape_feature = 32\n",
    "# from tensorflow.keras import regularizers\n",
    "model_feature = Sequential()\n",
    "model_feature.add(Dense(input_shape=(input_shape_feature), units=input_shape_feature, activation='relu'))\n",
    "model_feature.add(Dense(units=128, \n",
    "#                         kernel_regularizer=regularizers.l2(0.1),\n",
    "#                         activity_regularizer=regularizers.l1(),\n",
    "                        activation='relu'))\n",
    "model_feature.add(Dense(units=32, activation='relu'))\n",
    "model_feature.add(Dense(units=1, activation='sigmoid'))\n",
    "model_feature.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "\n",
    "# model training\n",
    "model_feature.fit(X_trainer, Y_trainer, epochs=5, batch_size=8)\n",
    "\n",
    "# MODEL_PATH = \"/public/liujiawei/huawei/ZHB/ADF-master/models/\"\n",
    "# model.save(MODEL_PATH+\"fmnist_FC4_bad.h5\")\n",
    "\n",
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "c82e92b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_Gini_act = []\n",
    "for data1 in data:\n",
    "#     MLP_Gini_act.append(  model_feature.predict(np.reshape(data1, (1,-1)))[0][0]  )\n",
    "    MLP_Gini_act.append(  model_feature.predict(np.reshape(data1, (1,-1)))[0][0]  )\n",
    "\n",
    "indexs = np.argsort(MLP_Gini_act)\n",
    "# indexs = indexs[::-1]\n",
    "# print(indexs)\n",
    "APFD,wrong_number,wrong_index = get_APFD(Gini_indexs=indexs, ground_truth_label=Y_test, \n",
    "                predicted_confidence=np.array(predicted_confidence))\n",
    "print(\"APFD: \", APFD, wrong_number)\n",
    "RAUC,_,_ = get_RAUC(Gini_indexs=indexs, ground_truth_label=Y_test, \n",
    "                predicted_confidence=np.array(predicted_confidence))\n",
    "print(\"RAUC: \", RAUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "5ab8a869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30898452"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_feature.predict(np.reshape(data1, (1,-1)))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "53071caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac215a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "52bc65d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gini_act =[]\n",
    "pbar = ProgressBar()\n",
    "for i in pbar(range(len(grads_CE))):\n",
    "#     x_act = np.sum(np.abs(grads_CE[i]))\n",
    "    x_act =[np.sum(np.abs(grads_CE[i])),\n",
    "            np.sum(np.abs(grads_CE2[i])),\n",
    "#             np.sum(np.abs(grads_CE_min[i])),\n",
    "#             np.sum(np.abs(grads_RCE[i])),\n",
    "#             np.sum(np.abs(grads_CE_ave[i])),\n",
    "            np.sum(np.abs(grads_pre1[i])),\n",
    "            np.sum(np.abs(grads_pre2[i])),\n",
    "#             np.sum(np.abs(grads_pre_min[i]))\n",
    "           ]\n",
    "#     print(i,':',x_act)\n",
    "    Gini_act.append(x_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d6248789",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gini_act = np.array(Gini_act)\n",
    "indexs = np.argsort(Gini_act[:,0]*Gini_act[:,0])\n",
    "indexs = indexs[::-1]\n",
    "APFD,_,_ = get_APFD(Gini_indexs=indexs, ground_truth_label=Y_test, \n",
    "                predicted_confidence=np.array(predicted_confidence))\n",
    "print('APFD:', APFD)\n",
    "RAUC,_,_ = get_RAUC(Gini_indexs=indexs, ground_truth_label=Y_test, \n",
    "                predicted_confidence=np.array(predicted_confidence))\n",
    "print('RAUC:', RAUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9f1d7449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(grads_CE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8b8d24d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradients(img_input, model, top_pred_idx):\n",
    "    images = tf.cast(img_input, tf.float32)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(images)\n",
    "        preds = model(images)\n",
    "#         print(np.shape(images))\n",
    "#         print(np.shape(preds))\n",
    "        top_class = preds[:, top_pred_idx]\n",
    "\n",
    "    grads = tape.gradient(top_class, images)\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4726ff94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 784)               615440    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 720,378\n",
      "Trainable params: 720,378\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43982d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n",
      "100% |########################################################################|\n",
      "100% |########################################################################|\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APFD: 0.7498684210526315\n",
      "RAUC: 0.8690330477356181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# # use one layer's activation\n",
    "# get_activations = BE.function(inputs=model.inputs[0], outputs=model.layers[-1].output[:,:])\n",
    "# Gini_act = []\n",
    "# pbar = ProgressBar()\n",
    "# for x_tmp in pbar(X_test):    \n",
    "#     # 使用attention机制\n",
    "#     x_act = softmax(get_activations(x_tmp.reshape([-1,28*28])))\n",
    "#     att1 = np.dot(np.transpose(x_act), x_act)\n",
    "#     att_weight = np.sum(att1, axis = 1).reshape(np.shape(x_act))\n",
    "#     x_act_ = softmax(x_act*att_weight)\n",
    "#     Gini_tmp = 1-np.sum(x_act_*x_act_)\n",
    "#     Gini_tmp = -np.sum(x_act_*np.log2(x_act_))\n",
    "#     Gini_act.append(Gini_tmp)\n",
    "    \n",
    "#     # 未使用attention机制\n",
    "#     x_act = softmax(get_activations(x_tmp.reshape([-1,28*28])))\n",
    "#     Gini_tmp = 1-np.sum(x_act*x_act)\n",
    "# #     Gini_tmp = -np.sum(x_act*np.log2(x_act))\n",
    "#     Gini_act.append(Gini_tmp)\n",
    "\n",
    "# # use all layers' activation\n",
    "# pbar = ProgressBar()\n",
    "# get_activations1 = BE.function(inputs=model.inputs[0], outputs=model.layers[-1].output[:,:])\n",
    "# get_activations2 = BE.function(inputs=model.inputs[0], outputs=model.layers[-2].output[:,:])\n",
    "# get_activations3 = BE.function(inputs=model.inputs[0], outputs=model.layers[-3].output[:,:])\n",
    "# Gini_act = []\n",
    "# for x_tmp in pbar(X_test):\n",
    "#     x_act1 = softmax(get_activations1(x_tmp.reshape([-1,28*28])))\n",
    "#     x_act2 = softmax(get_activations2(x_tmp.reshape([-1,28*28])))\n",
    "#     x_act3 = softmax(get_activations3(x_tmp.reshape([-1,28*28])))\n",
    "# #     Gini_tmp = 1-np.sum(x_act3*x_act3)\n",
    "# #     Gini_tmp = 1-(np.sum(x_act1*x_act1)+np.sum(x_act2*x_act2)+np.sum(x_act3*x_act3))/3\n",
    "#     Gini_tmp = -np.sum(x_act3*np.log2(x_act3))\n",
    "# #     Gini_tmp = (-np.sum(x_act1*np.log2(x_act1))-np.sum(x_act2*np.log2(x_act2))-np.sum(x_act3*np.log2(x_act3)))/3\n",
    "#     Gini_act.append(Gini_tmp)\n",
    "    \n",
    "# indexs = np.argsort(Gini_act)\n",
    "# indexs = indexs[::-1]\n",
    "# o_i = 0\n",
    "# pbar = ProgressBar()\n",
    "# for i in pbar(range(0, (index1))):\n",
    "#     x_tmp = X_test[indexs[i]].reshape([-1,28*28])\n",
    "#     y_tmp = Y_test[indexs[i]]\n",
    "#     per_tmp = model.predict(x_tmp)\n",
    "#     if np.argmax(y_tmp)!=np.argmax(per_tmp):\n",
    "#         o_i = o_i+i\n",
    "# APFD = 1-o_i/(index1*(index1-index2))+1/(2*index1)\n",
    "# print(APFD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3854353e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ab7ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d156d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "APFD 指标基于基尼系数\n",
    " 使用所有层\n",
    "0.9357219241773963 使用倒数第三层\n",
    "0.946923104434907 使用倒数第二层\n",
    "0.921514556509299 使用倒数第一层，即最后一层（DeepGini）\n",
    "\n",
    "\n",
    "APFD 指标基于信息熵\n",
    "0.9608580114449213 使用所有层\n",
    "0.9608580114449213 使用倒数第三层\n",
    "0.9326023962804005 使用倒数第二层\n",
    "0.9212451001430615 使用倒数第一层，即最后一层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedbb8c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
