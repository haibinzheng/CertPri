{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fd64d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from Constant import *\n",
    "from Model_Structure import *\n",
    "from dataset import *\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.optimizer_v1 import Adam\n",
    "from keras.losses import categorical_crossentropy, sparse_categorical_crossentropy\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "970ba58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "X_train, Y_train, X_test, Y_test = load_radio2016_regress(mod_snr=('8PSK', 18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c0aed83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(688, 2, 127) (688, 2) (312, 2, 127) (312, 2)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train), np.shape(Y_train), np.shape(X_test), np.shape(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed85f844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "inshape = (2, 127)\n",
    "input = Input(shape=inshape)\n",
    "output = LSTM(units=256, activation=\"tanh\", return_sequences=True)(input)\n",
    "output = LSTM(units=128, activation=\"tanh\", return_sequences=True)(output)\n",
    "\n",
    "output = Flatten()(output)\n",
    "\n",
    "output = Dense(1024, activation=None)(output)\n",
    "output = BatchNormalization()(output)\n",
    "output = Activation(activation=\"relu\")(output)\n",
    "output = Dropout(0.5)(output)\n",
    "\n",
    "output = Dense(128, activation=None)(output)\n",
    "output = BatchNormalization()(output)\n",
    "output = Activation(activation=\"relu\")(output)\n",
    "output = Dropout(0.5)(output)\n",
    "\n",
    "output = Dense(16, activation=None)(output)\n",
    "output = BatchNormalization()(output)\n",
    "output = Activation(activation=\"relu\")(output)\n",
    "output = Dropout(0.5)(output)\n",
    "\n",
    "output1 = Dense(2, activation='sigmoid')(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba2c0278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 2, 127)]          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 2, 256)            393216    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 2, 128)            197120    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1024)              263168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                2064      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 991,474\n",
      "Trainable params: 989,138\n",
      "Non-trainable params: 2,336\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(input, output1)\n",
    "optimizer = Adam(lr=1e-4)\n",
    "model.compile(optimizer=optimizer, loss=\"mse\", metrics=[\"mse\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efcc1f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 688 samples, validate on 312 samples\n",
      "Epoch 1/10\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.0834 - mse: 0.0834"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/public/liujiawei/anaconda3/envs/ZHB_env/lib/python3.6/site-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01110, saving model to Weights/signal.h5\n",
      "688/688 [==============================] - 5s 8ms/sample - loss: 0.0834 - mse: 0.0834 - val_loss: 0.0111 - val_mse: 0.0111 - lr: 1.0000e-04\n",
      "Epoch 2/10\n",
      "664/688 [===========================>..] - ETA: 0s - loss: 0.0824 - mse: 0.0824\n",
      "Epoch 00002: val_loss did not improve from 0.01110\n",
      "688/688 [==============================] - 1s 2ms/sample - loss: 0.0824 - mse: 0.0824 - val_loss: 0.0125 - val_mse: 0.0125 - lr: 1.0000e-04\n",
      "Epoch 3/10\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.0734 - mse: 0.0734\n",
      "Epoch 00003: val_loss did not improve from 0.01110\n",
      "688/688 [==============================] - 2s 2ms/sample - loss: 0.0734 - mse: 0.0734 - val_loss: 0.0138 - val_mse: 0.0138 - lr: 1.0000e-04\n",
      "Epoch 4/10\n",
      "688/688 [==============================] - ETA: 0s - loss: 0.0715 - mse: 0.0715\n",
      "Epoch 00004: val_loss did not improve from 0.01110\n",
      "688/688 [==============================] - 1s 2ms/sample - loss: 0.0715 - mse: 0.0715 - val_loss: 0.0130 - val_mse: 0.0130 - lr: 3.1623e-05\n",
      "Epoch 5/10\n",
      "664/688 [===========================>..] - ETA: 0s - loss: 0.0651 - mse: 0.0651\n",
      "Epoch 00005: val_loss did not improve from 0.01110\n",
      "688/688 [==============================] - 1s 2ms/sample - loss: 0.0658 - mse: 0.0658 - val_loss: 0.0131 - val_mse: 0.0131 - lr: 3.1623e-05\n",
      "Epoch 6/10\n",
      "672/688 [============================>.] - ETA: 0s - loss: 0.0668 - mse: 0.0668\n",
      "Epoch 00006: val_loss did not improve from 0.01110\n",
      "688/688 [==============================] - 1s 2ms/sample - loss: 0.0669 - mse: 0.0669 - val_loss: 0.0143 - val_mse: 0.0143 - lr: 1.0000e-05\n",
      "Epoch 7/10\n",
      "680/688 [============================>.] - ETA: 0s - loss: 0.0686 - mse: 0.0686\n",
      "Epoch 00007: val_loss did not improve from 0.01110\n",
      "688/688 [==============================] - 1s 2ms/sample - loss: 0.0690 - mse: 0.0690 - val_loss: 0.0177 - val_mse: 0.0177 - lr: 1.0000e-05\n",
      "Epoch 8/10\n",
      "672/688 [============================>.] - ETA: 0s - loss: 0.0700 - mse: 0.0700\n",
      "Epoch 00008: val_loss did not improve from 0.01110\n",
      "688/688 [==============================] - 1s 2ms/sample - loss: 0.0702 - mse: 0.0702 - val_loss: 0.0257 - val_mse: 0.0257 - lr: 1.0000e-05\n",
      "Epoch 9/10\n",
      "664/688 [===========================>..] - ETA: 0s - loss: 0.0652 - mse: 0.0652\n",
      "Epoch 00009: val_loss did not improve from 0.01110\n",
      "688/688 [==============================] - 1s 2ms/sample - loss: 0.0658 - mse: 0.0658 - val_loss: 0.0365 - val_mse: 0.0365 - lr: 1.0000e-05\n",
      "Epoch 10/10\n",
      "672/688 [============================>.] - ETA: 0s - loss: 0.0647 - mse: 0.0647\n",
      "Epoch 00010: val_loss did not improve from 0.01110\n",
      "688/688 [==============================] - 1s 2ms/sample - loss: 0.0645 - mse: 0.0645 - val_loss: 0.0420 - val_mse: 0.0420 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5af85966a0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 8\n",
    "epochs = 10\n",
    "# 加载模型\n",
    "# model = radio2016_model(lr=1e-4)\n",
    "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=np.sqrt(0.1), cooldown=0, patience=2, min_lr=1e-5)\n",
    "model_checkpoint = ModelCheckpoint(\"Weights/signal.h5\", monitor=\"val_loss\",\n",
    "                                   save_best_only=True, save_weights_only=False, verbose=1)\n",
    "callbacks = [model_checkpoint, lr_reducer]\n",
    "\n",
    "model.fit(x=X_train, y=Y_train, batch_size=batch_size, epochs=epochs, shuffle=True,\n",
    "          callbacks=callbacks, validation_data=(X_test, Y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f825f18e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.028607895597815514, 0.028607896]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d7623fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.04204105595365549, 0.042041056]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b747b95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
